{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Brazilian Weather Data","text":"<p>The Brazil Weather Data project is a purely educational initiative aimed at transforming the data from Brazilian automatic weather stations into a lightweight API. This project serves as an excellent resource for learning and experimentation with API development, data handling, and modern software engineering practices.</p> <p>This API is hosted in a free tier webservice on Render. Due the resource avaliable, the data is ingested in a local machine and only upload to the repository.</p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>app/\n    main.py                 # Contains the routes for the API\n    tools/                  # Tools for data loading and manipulation\n    general                 # General utility tools\n    validators              # Tools for ensuring data quality and integrity\n    collectors              # Modules for data collection\n    pipeline                # The complete pipeline for loading data into the database\ntests/\n    test_api                # Tests for API routes\n    tools/                  # Tests for various utility tools\n    test_general            # Tests for general utility tools\n    test_validators         # Tests for data validation tools\n    test_collectors         # Tests for data collection modules\n</code></pre>"},{"location":"app/api/","title":"API &gt; Routes","text":"<p>Main API module.</p>"},{"location":"app/api/#app.main.get_weather_data","title":"<code>get_weather_data(id_station_who, start_date, end_date)</code>","text":"<p>Return information about weather.</p> <p>Return information about weather collected by a station between a start and end date. The interval between start and end date should be less than five weeks.</p> Column Portuguese Description English Description Measure Unit Date Data Date - UTCTime Hora UTC UTCTime - TotalPrecipitation Precipita\u00e7\u00e3o total no hor\u00e1rio Total Precipitation at Time mm AtmosphericPressure Press\u00e3o atmosf\u00e9rica ao n\u00edvel da esta\u00e7\u00e3o, hor\u00e1ria Hourly Atmospheric Pressure mB MaxAtmosphericPressure Press\u00e3o atmosf\u00e9rica m\u00e1xima na hora anterior (autom\u00e1tica) Max Atmospheric Pressure (Auto) mB MinAtmosphericPressure Press\u00e3o atmosf\u00e9rica m\u00ednima na hora anterior (autom\u00e1tica) Min Atmospheric Pressure (Auto) mB GlobalRadiation Radia\u00e7\u00e3o global Global Radiation Kj/m\u00b2 DryBulbTemperature Temperatura do ar - bulbo seco, hor\u00e1ria Dry Bulb Air Temperature \u00b0C DewPointTemperature Temperatura do ponto de orvalho Dew Point Temperature \u00b0C MaxTemperaturePreviousHour Temperatura m\u00e1xima na hora anterior (autom\u00e1tica) Max Temperature Previous Hour \u00b0C MinTemperaturePreviousHour Temperatura m\u00ednima na hora anterior (autom\u00e1tica) Min Temperature Previous Hour \u00b0C MaxDewPointTemperature Temperatura de orvalho m\u00e1xima na hora anterior (auto) Max Dew Point Temperature (Auto) \u00b0C MinDewPointTemperature Temperatura de orvalho m\u00ednima na hora anterior (auto) Min Dew Point Temperature (Auto) \u00b0C MaxRelativeHumidity Umidade relativa m\u00e1xima na hora anterior (autom\u00e1tica) Max Relative Humidity (Auto) % MinRelativeHumidity Umidade relativa m\u00ednima na hora anterior (autom\u00e1tica) Min Relative Humidity (Auto) % HourlyRelativeHumidity Umidade relativa do ar, hor\u00e1ria Hourly Relative Humidity % WindDirectionDegrees Vento, dire\u00e7\u00e3o hor\u00e1ria Hourly Wind Direction \u00b0 (gr) MaxWindGust Vento, rajada m\u00e1xima Max Wind Gust m/s WindSpeed Vento, velocidade hor\u00e1ria Hourly Wind Speed m/s Source code in <code>app/main.py</code> <pre><code>@app.get(\"/weather/{id_station_who}/{start_date}/{end_date}/\", tags=[\"weather\"])  # noqa\ndef get_weather_data(\n    id_station_who: IdStationWhoType,\n    start_date: PastDate,\n    end_date: PastDate,\n):\n    \"\"\"Return information about weather.\n\n    Return information about weather collected by a station between a start\n    and end date. The interval between start and end date should be less\n    than five weeks.\n\n    | Column                       | Portuguese Description                                   | English Description               | Measure Unit      |\n    |------------------------------|----------------------------------------------------------|-----------------------------------|-------------------|\n    | Date                         | Data                                                     | Date                              | -                 |\n    | UTCTime                      | Hora UTC                                                 | UTCTime                           | -                 |\n    | TotalPrecipitation           | Precipita\u00e7\u00e3o total no hor\u00e1rio                            | Total Precipitation at Time       | mm                |\n    | AtmosphericPressure          | Press\u00e3o atmosf\u00e9rica ao n\u00edvel da esta\u00e7\u00e3o, hor\u00e1ria         | Hourly Atmospheric Pressure       | mB                |\n    | MaxAtmosphericPressure       | Press\u00e3o atmosf\u00e9rica m\u00e1xima na hora anterior (autom\u00e1tica) | Max Atmospheric Pressure (Auto)   | mB                |\n    | MinAtmosphericPressure       | Press\u00e3o atmosf\u00e9rica m\u00ednima na hora anterior (autom\u00e1tica) | Min Atmospheric Pressure (Auto)   | mB                |\n    | GlobalRadiation              | Radia\u00e7\u00e3o global                                          | Global Radiation                  | Kj/m\u00b2             |\n    | DryBulbTemperature           | Temperatura do ar - bulbo seco, hor\u00e1ria                  | Dry Bulb Air Temperature          | \u00b0C                |\n    | DewPointTemperature          | Temperatura do ponto de orvalho                          | Dew Point Temperature             | \u00b0C                |\n    | MaxTemperaturePreviousHour   | Temperatura m\u00e1xima na hora anterior (autom\u00e1tica)         | Max Temperature Previous Hour     | \u00b0C                |\n    | MinTemperaturePreviousHour   | Temperatura m\u00ednima na hora anterior (autom\u00e1tica)         | Min Temperature Previous Hour     | \u00b0C                |\n    | MaxDewPointTemperature       | Temperatura de orvalho m\u00e1xima na hora anterior (auto)    | Max Dew Point Temperature (Auto)  | \u00b0C                |\n    | MinDewPointTemperature       | Temperatura de orvalho m\u00ednima na hora anterior (auto)    | Min Dew Point Temperature (Auto)  | \u00b0C                |\n    | MaxRelativeHumidity          | Umidade relativa m\u00e1xima na hora anterior (autom\u00e1tica)    | Max Relative Humidity (Auto)      | %                 |\n    | MinRelativeHumidity          | Umidade relativa m\u00ednima na hora anterior (autom\u00e1tica)    | Min Relative Humidity (Auto)      | %                 |\n    | HourlyRelativeHumidity       | Umidade relativa do ar, hor\u00e1ria                          | Hourly Relative Humidity          | %                 |\n    | WindDirectionDegrees         | Vento, dire\u00e7\u00e3o hor\u00e1ria                                   | Hourly Wind Direction             | \u00b0 (gr)            |\n    | MaxWindGust                  | Vento, rajada m\u00e1xima                                     | Max Wind Gust                     | m/s               |\n    | WindSpeed                    | Vento, velocidade hor\u00e1ria                                | Hourly Wind Speed                 | m/s               |\n\n\n    \"\"\"\n    if start_date &gt; end_date:\n        raise HTTPException(\n            status_code=422,\n            detail=\"end_date should be after start_date.\",\n        )\n\n    if end_date &gt; start_date + timedelta(weeks=5):\n        raise HTTPException(\n            status_code=422,\n            detail=\"Maximum period between start_date and end_date should be 5 weeks.\",  # noqa\n        )\n\n    query_sql = f\"\"\"\n                SELECT\n                    *\n                FROM\n                    '{weather_db}'\n                WHERE\n                    IdStationWho = '{id_station_who}' AND\n                    Date &gt;= '{start_date}' AND\n                    Date &lt;= '{end_date}'\n                \"\"\"  # nosec B608\n    result = query_db(query_sql)\n\n    if result:\n        return result\n    else:\n        raise HTTPException(\n            status_code=422,\n            detail=\"There is no data to show. Rewrite your query.\",\n        )\n</code></pre>"},{"location":"app/api/#app.main.list_station","title":"<code>list_station(id_station_who)</code>","text":"<p>Return information about a selected station using IdStationWho.</p> Source code in <code>app/main.py</code> <pre><code>@app.get(\"/stations/{id_station_who}/\", tags=[\"stations\"])\ndef list_station(\n    id_station_who: IdStationWhoType,\n):\n    \"\"\"Return information about a selected station using IdStationWho.\"\"\"\n    query_sql = f\"\"\"SELECT *\n                    FROM '{stations_db}'\n                    WHERE IdStationWho = '{id_station_who}'\"\"\"  # nosec B608\n    result = query_db(query_sql)\n    if result:\n        return result\n    else:\n        raise HTTPException(status_code=404, detail=\"Station do not exist.\")\n</code></pre>"},{"location":"app/api/#app.main.list_stations","title":"<code>list_stations()</code>","text":"<p>Return the following information about all the stations.</p> <ul> <li>IdStationWho</li> <li>Region</li> <li>State</li> <li>StationName</li> <li>Latitude</li> <li>Longitude</li> <li>Altitude</li> <li>FoundingDate</li> </ul> <p>Future. This information can be used in the query route to build the SQL statements.</p> Source code in <code>app/main.py</code> <pre><code>@app.get(\"/stations/\", tags=[\"stations\"])\ndef list_stations():\n    \"\"\"\n    Return the following information about all the stations.\n\n    - IdStationWho\n    - Region\n    - State\n    - StationName\n    - Latitude\n    - Longitude\n    - Altitude\n    - FoundingDate\n\n    *Future.* This information can be used in the query route to build the SQL\n    statements.\n    \"\"\"\n    query_sql = f\"SELECT * FROM '{stations_db}'\"  # nosec B608\n    return query_db(query_sql)\n</code></pre>"},{"location":"app/api/#app.main.query_db","title":"<code>query_db(sql_query)</code>","text":"<p>Execute a SQL query and return the results in JSON format.</p> <p>This function takes a SQL query string, executes it using DuckDB, and returns the results as a JSON array. It's designed to provide an easy interface for querying a database and getting the results in a web-friendly format. The response is formatted with 'records' orientation and ISO date format.</p>"},{"location":"app/api/#app.main.query_db--parameters","title":"Parameters","text":"<p>sql_query : str     A SQL query string to be executed in the DuckDB database.</p>"},{"location":"app/api/#app.main.query_db--returns","title":"Returns","text":"<p>list     A list of dictionaries representing the rows of the query result.     Each dictionary corresponds to a row, with column names as keys.</p>"},{"location":"app/api/#app.main.query_db--raises","title":"Raises","text":"<p>Exception     Raises an exception if the SQL query execution or JSON conversion     fails.</p> Source code in <code>app/main.py</code> <pre><code>def query_db(sql_query: str) -&gt; list:\n    \"\"\"\n    Execute a SQL query and return the results in JSON format.\n\n    This function takes a SQL query string, executes it using DuckDB, and\n    returns the results as a JSON array. It's designed to provide an easy\n    interface for querying a database and getting the results in a\n    web-friendly format. The response is formatted with 'records' orientation\n    and ISO date format.\n\n    Parameters\n    ----------\n    sql_query : str\n        A SQL query string to be executed in the DuckDB database.\n\n    Returns\n    -------\n    list\n        A list of dictionaries representing the rows of the query result.\n        Each dictionary corresponds to a row, with column names as keys.\n\n    Raises\n    ------\n    Exception\n        Raises an exception if the SQL query execution or JSON conversion\n        fails.\n    \"\"\"\n    try:\n        response = (\n            duckdb.sql(sql_query)\n            .df()\n            .to_json(\n                orient=\"records\",\n                date_format=\"iso\",\n            )\n        )\n        response_json = json.loads(response)\n        return response_json\n    except Exception as e:\n        raise Exception(f\"Error in query_db function: {e}\")\n</code></pre>"},{"location":"app/api/#app.main.root","title":"<code>root()</code>","text":"<p>Redirect user to API documentation.</p> Source code in <code>app/main.py</code> <pre><code>@app.get(\"/\")\ndef root() -&gt; None:\n    \"\"\"Redirect user to API documentation.\"\"\"\n    return RedirectResponse(url=\"/docs\")\n</code></pre>"},{"location":"app/tools/collectors/","title":"Tools &gt; Collectors","text":"<p>Data Collectors.</p> <p>This module provides utility functions for data collection.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector","title":"<code>StationDataCollector</code>","text":"<p>Data collector for weather stations in Brazil.</p> <p>This class is designed to collect, validate, transform, and save data from CSV files related to weather stations across Brazil. It utilizes a Pydantic model for data validation to ensure the integrity and accuracy of the data collected.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector--parameters","title":"Parameters:","text":"<p>input_folder : str     Directory containing the source CSV files. output_path : str     Directory where processed files will be saved. file_name : str     Base name for the output file. column_names : Dict[str, str]     Mapping from original column names to desired column names. schema : BaseModel     Pydantic model used for validating the data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector--attributes","title":"Attributes:","text":"<p>_input_folder : str     Input directory for source CSV files. _output_path : str     Output directory for processed data. _file_name : str     Base name for the output file. _schema : BaseModel     Pydantic model for data validation. _column_names : Dict[str, str]     Column name mapping.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>class StationDataCollector:\n    \"\"\"Data collector for weather stations in Brazil.\n\n    This class is designed to collect, validate, transform, and save data from\n    CSV files related to weather stations across Brazil. It utilizes a\n    Pydantic model for data validation to ensure the integrity and accuracy\n    of the data collected.\n\n    Parameters:\n    ----------\n    input_folder : str\n        Directory containing the source CSV files.\n    output_path : str\n        Directory where processed files will be saved.\n    file_name : str\n        Base name for the output file.\n    column_names : Dict[str, str]\n        Mapping from original column names to desired column names.\n    schema : BaseModel\n        Pydantic model used for validating the data.\n\n    Attributes:\n    ----------\n    _input_folder : str\n        Input directory for source CSV files.\n    _output_path : str\n        Output directory for processed data.\n    _file_name : str\n        Base name for the output file.\n    _schema : BaseModel\n        Pydantic model for data validation.\n    _column_names : Dict[str, str]\n        Column name mapping.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_folder: str,\n        output_path: str,\n        file_name: str,\n        column_names: Dict[str, str],\n        schema,\n    ):\n        \"\"\"Initialize a new StationDataCollector instance.\n\n        Parameters:\n        ----------\n        input_folder : str\n            Directory containing the source CSV files.\n        output_path : str\n            Directory where processed files will be saved.\n        file_name : str\n            Base name for the output file.\n        column_names : Dict[str, str]\n            Mapping from original column names to desired column names.\n        schema : BaseModel\n            Pydantic model used for validating the data.\n        \"\"\"\n        self._input_folder = input_folder\n        self._output_path = output_path\n        self._file_name = file_name\n        self._schema = schema\n        self._column_names = column_names\n\n    def start(self):\n        \"\"\"Initiate the data collection process.\n\n        This method orchestrates the workflow of data collection including\n        retrieving, validating, transforming, and storing data.\n        \"\"\"\n        response, data_files = self.get_data()\n\n        self.validate_data(response)\n\n        response = self.transform_data(\n            response,\n            self._column_names,\n            self._output_path,\n            data_files,\n        )\n\n        self.load_data(\n            response,\n            self._output_path,\n            self._file_name,\n        )\n\n    @logger_decorator\n    def get_data(self) -&gt; Tuple[List[pd.DataFrame], List[str]]:\n        \"\"\"Get data from CSV files.\n\n        Returns:\n        -------\n        tuple[List[pd.DataFrame], List[str]]\n            A tuple containing a list of DataFrames representing the data and\n            a list of strings representing the file names.\n\n        Raises:\n        ------\n        ValueError\n            If no CSV files are found in the specified directory.\n        \"\"\"\n        # Collecting all CSV file paths from the input folder\n        data_files = glob.glob(os.path.join(self._input_folder, \"*.csv\"))\n        data_files.extend(glob.glob(os.path.join(self._input_folder, \"*.CSV\")))\n        if not data_files:\n            raise ValueError(\"No CSV files found in the specified folder.\")\n\n        all_data = [\n            pd.read_csv(\n                file,\n                encoding=\"latin-1\",\n                nrows=8,\n                sep=\";\",\n                header=None,\n                decimal=\",\",\n                index_col=0,\n            ).T\n            for file in data_files[:100]\n        ]\n\n        return all_data, data_files\n\n    @logger_decorator\n    def validate_data(self, all_data: List[pd.DataFrame]) -&gt; None:\n        \"\"\"Validate the structure of the collected data.\n\n        Parameters:\n        ----------\n        all_data : List[pd.DataFrame]\n            List of DataFrames to be validated.\n\n        Raises:\n        ------\n        ValueError\n            If the structure of any DataFrame is inconsistent.\n        \"\"\"\n        list_columns = [df.columns.to_list() for df in all_data]\n        validate_sublists(list_columns)\n\n    @logger_decorator\n    def transform_data(\n        self,\n        all_data: List[pd.DataFrame],\n        column_names: Dict[str, str],\n        output_path: str,\n        data_files: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Transform and validate the collected data.\n\n        Parameters:\n        ----------\n        all_data : List[pd.DataFrame]\n            List of DataFrames to be transformed.\n        column_names : Dict[str, str]\n            Mapping of original column names to new names.\n        output_path : str\n            Path to the folder where the processed data will be saved.\n        data_files : List[str]\n            List of file names being processed.\n\n        Returns:\n        -------\n        pd.DataFrame\n            Concatenated and validated DataFrame.\n\n        Raises:\n        ------\n        Exception\n            If all collected data is invalid.\n        \"\"\"\n        files_processing = len(all_data)\n        logger.info(f\"Total files to process = {files_processing:,}\")\n\n        logger.info(\"Analyzing data quality\")\n        process_data = []\n        good_df = pd.DataFrame()\n        for i, df in enumerate(all_data):\n            logger.info(f\"Files to process: {files_processing-i:,}\")\n\n            file_name_process = data_files[i].split(\"/\")[-1]\n            file_log = file_name_process.replace(\".\", \"_\")\n\n            df = df.rename(columns=column_names)\n            good_data = list(\n                validate_data_quality(\n                    df,\n                    output_path,\n                    file_log,\n                    self._schema,\n                ),\n            )\n            if good_data:\n                good_df = pd.DataFrame(good_data)\n                process_data.append(good_df)\n            else:\n                good_df = pd.DataFrame()\n\n        logger.info(\"Data validation finished\")\n\n        # Avoiding concatenating empty dataframes and warnings from Pandas\n        process_data_filtered = [\n            df for df in process_data if not df.empty and not df.isna().all().all()\n        ]\n        if process_data_filtered:\n            validate_data = pd.concat(process_data_filtered)\n            validate_data = validate_data.drop_duplicates([\"IdStationWho\"])\n            return validate_data\n        else:\n            raise Exception(\"All collected data was invalid.\")\n\n    @logger_decorator\n    def load_data(\n        self,\n        validate_data: pd.DataFrame,\n        load_path: str,\n        file_name: str,\n    ) -&gt; None:\n        \"\"\"Save the validated data to a Parquet file.\n\n        Parameters:\n        ----------\n        validate_data : pd.DataFrame\n            DataFrame containing validated station data.\n        load_path : str\n            Path to the folder where the Parquet file will be saved.\n        file_name : str\n            Name of the output file without the extension.\n\n        Raises:\n        ------\n        Exception\n            If there is an error converting data to Parquet.\n        \"\"\"\n        try:\n            validate_data.to_parquet(\n                os.path.join(\n                    load_path,\n                    file_name + \".parquet\",\n                ),\n                index=False,\n            )\n        except Exception as e:\n            print(f\"Error to convert data to parquet: {e}\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.__init__","title":"<code>__init__(input_folder, output_path, file_name, column_names, schema)</code>","text":"<p>Initialize a new StationDataCollector instance.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.__init__--parameters","title":"Parameters:","text":"<p>input_folder : str     Directory containing the source CSV files. output_path : str     Directory where processed files will be saved. file_name : str     Base name for the output file. column_names : Dict[str, str]     Mapping from original column names to desired column names. schema : BaseModel     Pydantic model used for validating the data.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def __init__(\n    self,\n    input_folder: str,\n    output_path: str,\n    file_name: str,\n    column_names: Dict[str, str],\n    schema,\n):\n    \"\"\"Initialize a new StationDataCollector instance.\n\n    Parameters:\n    ----------\n    input_folder : str\n        Directory containing the source CSV files.\n    output_path : str\n        Directory where processed files will be saved.\n    file_name : str\n        Base name for the output file.\n    column_names : Dict[str, str]\n        Mapping from original column names to desired column names.\n    schema : BaseModel\n        Pydantic model used for validating the data.\n    \"\"\"\n    self._input_folder = input_folder\n    self._output_path = output_path\n    self._file_name = file_name\n    self._schema = schema\n    self._column_names = column_names\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.get_data","title":"<code>get_data()</code>","text":"<p>Get data from CSV files.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.get_data--returns","title":"Returns:","text":"<p>tuple[List[pd.DataFrame], List[str]]     A tuple containing a list of DataFrames representing the data and     a list of strings representing the file names.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.get_data--raises","title":"Raises:","text":"<p>ValueError     If no CSV files are found in the specified directory.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef get_data(self) -&gt; Tuple[List[pd.DataFrame], List[str]]:\n    \"\"\"Get data from CSV files.\n\n    Returns:\n    -------\n    tuple[List[pd.DataFrame], List[str]]\n        A tuple containing a list of DataFrames representing the data and\n        a list of strings representing the file names.\n\n    Raises:\n    ------\n    ValueError\n        If no CSV files are found in the specified directory.\n    \"\"\"\n    # Collecting all CSV file paths from the input folder\n    data_files = glob.glob(os.path.join(self._input_folder, \"*.csv\"))\n    data_files.extend(glob.glob(os.path.join(self._input_folder, \"*.CSV\")))\n    if not data_files:\n        raise ValueError(\"No CSV files found in the specified folder.\")\n\n    all_data = [\n        pd.read_csv(\n            file,\n            encoding=\"latin-1\",\n            nrows=8,\n            sep=\";\",\n            header=None,\n            decimal=\",\",\n            index_col=0,\n        ).T\n        for file in data_files[:100]\n    ]\n\n    return all_data, data_files\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.load_data","title":"<code>load_data(validate_data, load_path, file_name)</code>","text":"<p>Save the validated data to a Parquet file.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.load_data--parameters","title":"Parameters:","text":"<p>validate_data : pd.DataFrame     DataFrame containing validated station data. load_path : str     Path to the folder where the Parquet file will be saved. file_name : str     Name of the output file without the extension.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.load_data--raises","title":"Raises:","text":"<p>Exception     If there is an error converting data to Parquet.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef load_data(\n    self,\n    validate_data: pd.DataFrame,\n    load_path: str,\n    file_name: str,\n) -&gt; None:\n    \"\"\"Save the validated data to a Parquet file.\n\n    Parameters:\n    ----------\n    validate_data : pd.DataFrame\n        DataFrame containing validated station data.\n    load_path : str\n        Path to the folder where the Parquet file will be saved.\n    file_name : str\n        Name of the output file without the extension.\n\n    Raises:\n    ------\n    Exception\n        If there is an error converting data to Parquet.\n    \"\"\"\n    try:\n        validate_data.to_parquet(\n            os.path.join(\n                load_path,\n                file_name + \".parquet\",\n            ),\n            index=False,\n        )\n    except Exception as e:\n        print(f\"Error to convert data to parquet: {e}\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.start","title":"<code>start()</code>","text":"<p>Initiate the data collection process.</p> <p>This method orchestrates the workflow of data collection including retrieving, validating, transforming, and storing data.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def start(self):\n    \"\"\"Initiate the data collection process.\n\n    This method orchestrates the workflow of data collection including\n    retrieving, validating, transforming, and storing data.\n    \"\"\"\n    response, data_files = self.get_data()\n\n    self.validate_data(response)\n\n    response = self.transform_data(\n        response,\n        self._column_names,\n        self._output_path,\n        data_files,\n    )\n\n    self.load_data(\n        response,\n        self._output_path,\n        self._file_name,\n    )\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.transform_data","title":"<code>transform_data(all_data, column_names, output_path, data_files)</code>","text":"<p>Transform and validate the collected data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.transform_data--parameters","title":"Parameters:","text":"<p>all_data : List[pd.DataFrame]     List of DataFrames to be transformed. column_names : Dict[str, str]     Mapping of original column names to new names. output_path : str     Path to the folder where the processed data will be saved. data_files : List[str]     List of file names being processed.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.transform_data--returns","title":"Returns:","text":"<p>pd.DataFrame     Concatenated and validated DataFrame.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.transform_data--raises","title":"Raises:","text":"<p>Exception     If all collected data is invalid.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef transform_data(\n    self,\n    all_data: List[pd.DataFrame],\n    column_names: Dict[str, str],\n    output_path: str,\n    data_files: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"Transform and validate the collected data.\n\n    Parameters:\n    ----------\n    all_data : List[pd.DataFrame]\n        List of DataFrames to be transformed.\n    column_names : Dict[str, str]\n        Mapping of original column names to new names.\n    output_path : str\n        Path to the folder where the processed data will be saved.\n    data_files : List[str]\n        List of file names being processed.\n\n    Returns:\n    -------\n    pd.DataFrame\n        Concatenated and validated DataFrame.\n\n    Raises:\n    ------\n    Exception\n        If all collected data is invalid.\n    \"\"\"\n    files_processing = len(all_data)\n    logger.info(f\"Total files to process = {files_processing:,}\")\n\n    logger.info(\"Analyzing data quality\")\n    process_data = []\n    good_df = pd.DataFrame()\n    for i, df in enumerate(all_data):\n        logger.info(f\"Files to process: {files_processing-i:,}\")\n\n        file_name_process = data_files[i].split(\"/\")[-1]\n        file_log = file_name_process.replace(\".\", \"_\")\n\n        df = df.rename(columns=column_names)\n        good_data = list(\n            validate_data_quality(\n                df,\n                output_path,\n                file_log,\n                self._schema,\n            ),\n        )\n        if good_data:\n            good_df = pd.DataFrame(good_data)\n            process_data.append(good_df)\n        else:\n            good_df = pd.DataFrame()\n\n    logger.info(\"Data validation finished\")\n\n    # Avoiding concatenating empty dataframes and warnings from Pandas\n    process_data_filtered = [\n        df for df in process_data if not df.empty and not df.isna().all().all()\n    ]\n    if process_data_filtered:\n        validate_data = pd.concat(process_data_filtered)\n        validate_data = validate_data.drop_duplicates([\"IdStationWho\"])\n        return validate_data\n    else:\n        raise Exception(\"All collected data was invalid.\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.validate_data","title":"<code>validate_data(all_data)</code>","text":"<p>Validate the structure of the collected data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.validate_data--parameters","title":"Parameters:","text":"<p>all_data : List[pd.DataFrame]     List of DataFrames to be validated.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.StationDataCollector.validate_data--raises","title":"Raises:","text":"<p>ValueError     If the structure of any DataFrame is inconsistent.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef validate_data(self, all_data: List[pd.DataFrame]) -&gt; None:\n    \"\"\"Validate the structure of the collected data.\n\n    Parameters:\n    ----------\n    all_data : List[pd.DataFrame]\n        List of DataFrames to be validated.\n\n    Raises:\n    ------\n    ValueError\n        If the structure of any DataFrame is inconsistent.\n    \"\"\"\n    list_columns = [df.columns.to_list() for df in all_data]\n    validate_sublists(list_columns)\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector","title":"<code>WeatherDataCollector</code>","text":"<p>Data collector for weather in Brazil.</p> <p>This class is designed to collect, validate, transform, and save data from CSV files related to weather stations across Brazil. It utilizes a Pydantic model for data validation to ensure the integrity and accuracy of the data collected.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector--parameters","title":"Parameters:","text":"<p>input_folder : str     Directory containing the source CSV files. output_path : str     Directory where processed files will be saved. file_name : str     Base name for the output file. column_names : Dict[str, str]     Mapping from original column names to desired column names. schema : BaseModel     Pydantic model used for validating the data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector--attributes","title":"Attributes:","text":"<p>_input_folder : str     Input directory for source CSV files. _output_path : str     Output directory for processed data. _file_name : str     Base name for the output file. _schema : BaseModel     Pydantic model for data validation. _column_names : Dict[str, str]     Column name mapping.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>class WeatherDataCollector:\n    \"\"\"Data collector for weather in Brazil.\n\n    This class is designed to collect, validate, transform, and save data from\n    CSV files related to weather stations across Brazil. It utilizes a\n    Pydantic model for data validation to ensure the integrity and accuracy\n    of the data collected.\n\n    Parameters:\n    ----------\n    input_folder : str\n        Directory containing the source CSV files.\n    output_path : str\n        Directory where processed files will be saved.\n    file_name : str\n        Base name for the output file.\n    column_names : Dict[str, str]\n        Mapping from original column names to desired column names.\n    schema : BaseModel\n        Pydantic model used for validating the data.\n\n    Attributes:\n    ----------\n    _input_folder : str\n        Input directory for source CSV files.\n    _output_path : str\n        Output directory for processed data.\n    _file_name : str\n        Base name for the output file.\n    _schema : BaseModel\n        Pydantic model for data validation.\n    _column_names : Dict[str, str]\n        Column name mapping.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_folder: str,\n        output_path: str,\n        file_name: str,\n        column_names: Dict[str, str],\n        schema,\n    ):\n        \"\"\"Initialize a new StationDataCollector instance.\n\n        Parameters:\n        ----------\n        input_folder : str\n            Directory containing the source CSV files.\n        output_path : str\n            Directory where processed files will be saved.\n        file_name : str\n            Base name for the output file.\n        column_names : Dict[str, str]\n            Mapping from original column names to desired column names.\n        schema : BaseModel\n            Pydantic model used for validating the data.\n        \"\"\"\n        self._input_folder = input_folder\n        self._output_path = output_path\n        self._file_name = file_name\n        self._schema = schema\n        self._column_names = column_names\n\n    def start(self):\n        \"\"\"Initiate the data collection process.\n\n        This method orchestrates the workflow of data collection including\n        retrieving, validating, transforming, and storing data.\n        \"\"\"\n        response, data_files = self.get_data()\n\n        self.validate_data(response)\n\n        response = self.transform_data(\n            response,\n            self._column_names,\n            self._output_path,\n            data_files,\n        )\n\n        self.load_data(\n            response,\n            self._output_path,\n            self._file_name,\n        )\n\n    @logger_decorator\n    def get_data(self) -&gt; tuple[List[pd.DataFrame], List[str]]:\n        \"\"\"Get data from CSV files.\n\n        Returns:\n        -------\n        tuple[List[pd.DataFrame], List[str]]\n            A tuple containing a list of DataFrames representing the data and\n            a list of strings representing the file names.\n\n        Raises:\n        ------\n        ValueError\n            If no CSV files are found in the specified directory.\n        \"\"\"\n        # Collecting all CSV file paths from the input folder\n        data_files = glob.glob(os.path.join(self._input_folder, \"*.csv\"))\n        data_files.extend(glob.glob(os.path.join(self._input_folder, \"*.CSV\")))\n        if not data_files:\n            raise ValueError(\"No CSV files found in the specified folder.\")\n\n        all_data = [\n            pd.read_csv(\n                file,\n                encoding=\"latin-1\",\n                skiprows=8,\n                sep=\";\",\n                usecols=[x for x in range(0, 19)],\n                na_values=[\"-9999\"],\n                dtype=str,\n            )\n            for file in data_files[:100]\n        ]\n\n        stations_data = [\n            pd.read_csv(\n                file,\n                encoding=\"latin-1\",\n                nrows=8,\n                sep=\";\",\n                header=None,\n                decimal=\",\",\n                index_col=0,\n            ).T\n            for file in data_files[:100]\n        ]\n\n        for i, df in enumerate(all_data):\n            df[\"IdStationWho\"] = stations_data[i].loc[\n                1,\n                \"CODIGO (WMO):\",\n            ]\n\n        return all_data, data_files\n\n    @logger_decorator\n    def validate_data(self, all_data: List[pd.DataFrame]) -&gt; None:\n        \"\"\"Validate the structure of the collected data.\n\n        Parameters:\n        ----------\n        all_data : List[pd.DataFrame]\n            List of DataFrames to be validated.\n\n        Raises:\n        ------\n        ValueError\n            If the structure of any DataFrame is inconsistent.\n        \"\"\"\n        list_columns = [df.columns.to_list() for df in all_data]\n        validate_sublists(list_columns)\n\n    @logger_decorator\n    def transform_data(\n        self,\n        all_data: List[pd.DataFrame],\n        column_names: Dict[str, str],\n        output_path: str,\n        data_files: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Transform and validate the collected data.\n\n        Parameters:\n        ----------\n        all_data : List[pd.DataFrame]\n            List of DataFrames to be transformed.\n        column_names : Dict[str, str]\n            Mapping of original column names to new names.\n        output_path : str\n            Path to the folder where the processed data will be saved.\n        data_files : List[str]\n            List of file names being processed.\n\n        Returns:\n        -------\n        pd.DataFrame\n            Concatenated and validated DataFrame.\n\n        Raises:\n        ------\n        Exception\n            If all collected data is invalid.\n        \"\"\"\n        files_processing = len(all_data)\n        logger.info(f\"Total files to process = {files_processing:,}\")\n\n        logger.info(\"Analyzing data quality\")\n        process_data = []\n        good_df = pd.DataFrame()\n        for i, df in enumerate(all_data):\n            logger.info(f\"Files to process: {files_processing-i:,}\")\n\n            file_name_process = data_files[i].split(\"/\")[-1]\n            file_log = file_name_process.replace(\".\", \"_\")\n\n            df = df.rename(columns=column_names)\n            good_data = list(\n                validate_data_quality(\n                    df,\n                    output_path,\n                    file_log,\n                    self._schema,\n                ),\n            )\n            if good_data:\n                good_df = pd.DataFrame(good_data)\n                process_data.append(good_df)\n            else:\n                good_df = pd.DataFrame()\n\n        logger.info(\"Data validation finished\")\n\n        # Avoiding concatenating empty dataframes and warnings from Pandas\n        process_data_filtered = [\n            df for df in process_data if not df.empty and not df.isna().all().all()\n        ]\n        if process_data_filtered:\n            validate_data = pd.concat(process_data_filtered)\n            return validate_data\n        else:\n            raise Exception(\"All collected data was invalid.\")\n\n    @logger_decorator\n    def load_data(\n        self,\n        validate_data: pd.DataFrame,\n        load_path: str,\n        file_name: str,\n    ) -&gt; None:\n        \"\"\"Save the validated data to a Parquet file.\n\n        Parameters:\n        ----------\n        validate_data : pd.DataFrame\n            DataFrame containing validated station data.\n        load_path : str\n            Path to the folder where the Parquet file will be saved.\n        file_name : str\n            Name of the output file without the extension.\n\n        Raises:\n        ------\n        Exception\n            If there is an error converting data to Parquet.\n        \"\"\"\n        try:\n            validate_data.to_parquet(\n                os.path.join(\n                    load_path,\n                    file_name + \".parquet\",\n                ),\n                index=False,\n            )\n        except Exception as e:\n            print(f\"Error to convert data to parquet: {e}\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.__init__","title":"<code>__init__(input_folder, output_path, file_name, column_names, schema)</code>","text":"<p>Initialize a new StationDataCollector instance.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.__init__--parameters","title":"Parameters:","text":"<p>input_folder : str     Directory containing the source CSV files. output_path : str     Directory where processed files will be saved. file_name : str     Base name for the output file. column_names : Dict[str, str]     Mapping from original column names to desired column names. schema : BaseModel     Pydantic model used for validating the data.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def __init__(\n    self,\n    input_folder: str,\n    output_path: str,\n    file_name: str,\n    column_names: Dict[str, str],\n    schema,\n):\n    \"\"\"Initialize a new StationDataCollector instance.\n\n    Parameters:\n    ----------\n    input_folder : str\n        Directory containing the source CSV files.\n    output_path : str\n        Directory where processed files will be saved.\n    file_name : str\n        Base name for the output file.\n    column_names : Dict[str, str]\n        Mapping from original column names to desired column names.\n    schema : BaseModel\n        Pydantic model used for validating the data.\n    \"\"\"\n    self._input_folder = input_folder\n    self._output_path = output_path\n    self._file_name = file_name\n    self._schema = schema\n    self._column_names = column_names\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.get_data","title":"<code>get_data()</code>","text":"<p>Get data from CSV files.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.get_data--returns","title":"Returns:","text":"<p>tuple[List[pd.DataFrame], List[str]]     A tuple containing a list of DataFrames representing the data and     a list of strings representing the file names.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.get_data--raises","title":"Raises:","text":"<p>ValueError     If no CSV files are found in the specified directory.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef get_data(self) -&gt; tuple[List[pd.DataFrame], List[str]]:\n    \"\"\"Get data from CSV files.\n\n    Returns:\n    -------\n    tuple[List[pd.DataFrame], List[str]]\n        A tuple containing a list of DataFrames representing the data and\n        a list of strings representing the file names.\n\n    Raises:\n    ------\n    ValueError\n        If no CSV files are found in the specified directory.\n    \"\"\"\n    # Collecting all CSV file paths from the input folder\n    data_files = glob.glob(os.path.join(self._input_folder, \"*.csv\"))\n    data_files.extend(glob.glob(os.path.join(self._input_folder, \"*.CSV\")))\n    if not data_files:\n        raise ValueError(\"No CSV files found in the specified folder.\")\n\n    all_data = [\n        pd.read_csv(\n            file,\n            encoding=\"latin-1\",\n            skiprows=8,\n            sep=\";\",\n            usecols=[x for x in range(0, 19)],\n            na_values=[\"-9999\"],\n            dtype=str,\n        )\n        for file in data_files[:100]\n    ]\n\n    stations_data = [\n        pd.read_csv(\n            file,\n            encoding=\"latin-1\",\n            nrows=8,\n            sep=\";\",\n            header=None,\n            decimal=\",\",\n            index_col=0,\n        ).T\n        for file in data_files[:100]\n    ]\n\n    for i, df in enumerate(all_data):\n        df[\"IdStationWho\"] = stations_data[i].loc[\n            1,\n            \"CODIGO (WMO):\",\n        ]\n\n    return all_data, data_files\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.load_data","title":"<code>load_data(validate_data, load_path, file_name)</code>","text":"<p>Save the validated data to a Parquet file.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.load_data--parameters","title":"Parameters:","text":"<p>validate_data : pd.DataFrame     DataFrame containing validated station data. load_path : str     Path to the folder where the Parquet file will be saved. file_name : str     Name of the output file without the extension.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.load_data--raises","title":"Raises:","text":"<p>Exception     If there is an error converting data to Parquet.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef load_data(\n    self,\n    validate_data: pd.DataFrame,\n    load_path: str,\n    file_name: str,\n) -&gt; None:\n    \"\"\"Save the validated data to a Parquet file.\n\n    Parameters:\n    ----------\n    validate_data : pd.DataFrame\n        DataFrame containing validated station data.\n    load_path : str\n        Path to the folder where the Parquet file will be saved.\n    file_name : str\n        Name of the output file without the extension.\n\n    Raises:\n    ------\n    Exception\n        If there is an error converting data to Parquet.\n    \"\"\"\n    try:\n        validate_data.to_parquet(\n            os.path.join(\n                load_path,\n                file_name + \".parquet\",\n            ),\n            index=False,\n        )\n    except Exception as e:\n        print(f\"Error to convert data to parquet: {e}\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.start","title":"<code>start()</code>","text":"<p>Initiate the data collection process.</p> <p>This method orchestrates the workflow of data collection including retrieving, validating, transforming, and storing data.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def start(self):\n    \"\"\"Initiate the data collection process.\n\n    This method orchestrates the workflow of data collection including\n    retrieving, validating, transforming, and storing data.\n    \"\"\"\n    response, data_files = self.get_data()\n\n    self.validate_data(response)\n\n    response = self.transform_data(\n        response,\n        self._column_names,\n        self._output_path,\n        data_files,\n    )\n\n    self.load_data(\n        response,\n        self._output_path,\n        self._file_name,\n    )\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.transform_data","title":"<code>transform_data(all_data, column_names, output_path, data_files)</code>","text":"<p>Transform and validate the collected data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.transform_data--parameters","title":"Parameters:","text":"<p>all_data : List[pd.DataFrame]     List of DataFrames to be transformed. column_names : Dict[str, str]     Mapping of original column names to new names. output_path : str     Path to the folder where the processed data will be saved. data_files : List[str]     List of file names being processed.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.transform_data--returns","title":"Returns:","text":"<p>pd.DataFrame     Concatenated and validated DataFrame.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.transform_data--raises","title":"Raises:","text":"<p>Exception     If all collected data is invalid.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef transform_data(\n    self,\n    all_data: List[pd.DataFrame],\n    column_names: Dict[str, str],\n    output_path: str,\n    data_files: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"Transform and validate the collected data.\n\n    Parameters:\n    ----------\n    all_data : List[pd.DataFrame]\n        List of DataFrames to be transformed.\n    column_names : Dict[str, str]\n        Mapping of original column names to new names.\n    output_path : str\n        Path to the folder where the processed data will be saved.\n    data_files : List[str]\n        List of file names being processed.\n\n    Returns:\n    -------\n    pd.DataFrame\n        Concatenated and validated DataFrame.\n\n    Raises:\n    ------\n    Exception\n        If all collected data is invalid.\n    \"\"\"\n    files_processing = len(all_data)\n    logger.info(f\"Total files to process = {files_processing:,}\")\n\n    logger.info(\"Analyzing data quality\")\n    process_data = []\n    good_df = pd.DataFrame()\n    for i, df in enumerate(all_data):\n        logger.info(f\"Files to process: {files_processing-i:,}\")\n\n        file_name_process = data_files[i].split(\"/\")[-1]\n        file_log = file_name_process.replace(\".\", \"_\")\n\n        df = df.rename(columns=column_names)\n        good_data = list(\n            validate_data_quality(\n                df,\n                output_path,\n                file_log,\n                self._schema,\n            ),\n        )\n        if good_data:\n            good_df = pd.DataFrame(good_data)\n            process_data.append(good_df)\n        else:\n            good_df = pd.DataFrame()\n\n    logger.info(\"Data validation finished\")\n\n    # Avoiding concatenating empty dataframes and warnings from Pandas\n    process_data_filtered = [\n        df for df in process_data if not df.empty and not df.isna().all().all()\n    ]\n    if process_data_filtered:\n        validate_data = pd.concat(process_data_filtered)\n        return validate_data\n    else:\n        raise Exception(\"All collected data was invalid.\")\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.validate_data","title":"<code>validate_data(all_data)</code>","text":"<p>Validate the structure of the collected data.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.validate_data--parameters","title":"Parameters:","text":"<p>all_data : List[pd.DataFrame]     List of DataFrames to be validated.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.WeatherDataCollector.validate_data--raises","title":"Raises:","text":"<p>ValueError     If the structure of any DataFrame is inconsistent.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>@logger_decorator\ndef validate_data(self, all_data: List[pd.DataFrame]) -&gt; None:\n    \"\"\"Validate the structure of the collected data.\n\n    Parameters:\n    ----------\n    all_data : List[pd.DataFrame]\n        List of DataFrames to be validated.\n\n    Raises:\n    ------\n    ValueError\n        If the structure of any DataFrame is inconsistent.\n    \"\"\"\n    list_columns = [df.columns.to_list() for df in all_data]\n    validate_sublists(list_columns)\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.collect_years_list","title":"<code>collect_years_list(list_years)</code>","text":"<p>Extract the valid years from a list.</p> <p>This function collects years beginning in 2000 and ending in the year from the last month. It filters out any years that are not integers or are outside the valid range.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.collect_years_list--parameters","title":"Parameters","text":"<p>list_years : List[int]     List with the years that will be processed.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.collect_years_list--returns","title":"Returns","text":"<p>List[int]     A list containing only the valid years within the specified range.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.collect_years_list--raises","title":"Raises","text":"<p>ValueError     If the list is empty or contains no valid years.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def collect_years_list(\n    list_years: List[int],\n) -&gt; List[int]:\n    \"\"\"Extract the valid years from a list.\n\n    This function collects years beginning in 2000 and ending in the\n    year from the last month. It filters out any years that are not\n    integers or are outside the valid range.\n\n    Parameters\n    ----------\n    list_years : List[int]\n        List with the years that will be processed.\n\n    Returns\n    -------\n    List[int]\n        A list containing only the valid years within the specified range.\n\n    Raises\n    ------\n    ValueError\n        If the list is empty or contains no valid years.\n    \"\"\"\n    first_year, last_year = limit_years()\n    valid_years = []\n    invalid_years = []\n\n    if len(list_years) == 0:\n        raise ValueError(\n            f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\",  # noqa\n        )\n\n    for year in list_years:\n        if not isinstance(year, int):\n            invalid_years.append(year)\n        elif first_year &lt;= year &lt;= last_year:\n            valid_years.append(year)\n        else:\n            invalid_years.append(year)\n\n    if len(valid_years) == 0:\n        raise ValueError(\n            f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\",  # noqa\n        )\n\n    if len(invalid_years) &gt; 0:\n        print(f\"The elements {invalid_years} were removed from the list.\")\n\n    return valid_years\n</code></pre>"},{"location":"app/tools/collectors/#app.tools.collectors.limit_years","title":"<code>limit_years()</code>","text":"<p>Calculate the valid year range for data collection.</p> <p>Determines the earliest year from which data can be collected and the latest valid year based on the current date.</p>"},{"location":"app/tools/collectors/#app.tools.collectors.limit_years--returns","title":"Returns","text":"<p>tuple[int, int]     A tuple containing two integers:     - The first integer is the earliest year from which data collection     is valid.     - The second integer is the latest valid year for data collection     based on the current date.</p> Source code in <code>app/tools/collectors.py</code> <pre><code>def limit_years() -&gt; tuple[int, int]:\n    \"\"\"Calculate the valid year range for data collection.\n\n    Determines the earliest year from which data can be collected and the\n    latest valid year based on the current date.\n\n    Returns\n    -------\n    tuple[int, int]\n        A tuple containing two integers:\n        - The first integer is the earliest year from which data collection\n        is valid.\n        - The second integer is the latest valid year for data collection\n        based on the current date.\n    \"\"\"\n    FIRST_YEAR_WITH_DATA = 2000\n    current_day = datetime.now().day\n    last_year_month = datetime.today() - timedelta(current_day + 1)\n    last_valid_year = last_year_month.year\n\n    return FIRST_YEAR_WITH_DATA, last_valid_year\n</code></pre>"},{"location":"app/tools/general/","title":"Tools &gt; General","text":"<p>General shared utilities.</p> <p>This module provides utility functions for common tasks such as downloading files from the internet and extract zip files.</p>"},{"location":"app/tools/general/#app.tools.general.clear_folder","title":"<code>clear_folder(folder_path)</code>","text":"<p>Remove all files and subdirectories within a specified folder.</p> <p>This function deletes all contents within the specified folder, including files and subdirectories, while keeping the folder itself intact.</p>"},{"location":"app/tools/general/#app.tools.general.clear_folder--parameters","title":"Parameters","text":"<p>folder_path : str     The path to the folder to be cleared.</p>"},{"location":"app/tools/general/#app.tools.general.clear_folder--raises","title":"Raises","text":"<p>FileNotFoundError     If the specified folder does not exist.</p> Source code in <code>app/tools/general.py</code> <pre><code>def clear_folder(\n    folder_path: str,\n) -&gt; None:\n    \"\"\"Remove all files and subdirectories within a specified folder.\n\n    This function deletes all contents within the specified folder, including\n    files and subdirectories, while keeping the folder itself intact.\n\n    Parameters\n    ----------\n    folder_path : str\n        The path to the folder to be cleared.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified folder does not exist.\n    \"\"\"\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"The folder {folder_path} does not exist.\")\n\n    for item in os.listdir(folder_path):\n        item_path = os.path.join(folder_path, item)\n        if os.path.isfile(item_path) or os.path.islink(item_path):\n            os.unlink(item_path)\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)\n</code></pre>"},{"location":"app/tools/general/#app.tools.general.download_file","title":"<code>download_file(url, file_name, save_path, chunk_size=128)</code>","text":"<p>Download a file from a given URL and save it to a specified path.</p>"},{"location":"app/tools/general/#app.tools.general.download_file--parameters","title":"Parameters","text":"<p>url : str     The URL of the file to be downloaded. file_name : str     The name of the file to be saved locally. save_path : str     The local folder where the downloaded file will be saved. chunk_size : int, optional     The size of chunks for downloading the file, defaults to 128 bytes.</p>"},{"location":"app/tools/general/#app.tools.general.download_file--returns","title":"Returns","text":"<p>None     The function writes the downloaded file to the specified location.</p>"},{"location":"app/tools/general/#app.tools.general.download_file--raises","title":"Raises","text":"<p>Exception     If an error occurs during the download or file writing process.</p> Source code in <code>app/tools/general.py</code> <pre><code>def download_file(\n    url: str,\n    file_name: str,\n    save_path: str,\n    chunk_size: int = 128,\n) -&gt; None:\n    \"\"\"\n    Download a file from a given URL and save it to a specified path.\n\n    Parameters\n    ----------\n    url : str\n        The URL of the file to be downloaded.\n    file_name : str\n        The name of the file to be saved locally.\n    save_path : str\n        The local folder where the downloaded file will be saved.\n    chunk_size : int, optional\n        The size of chunks for downloading the file, defaults to 128 bytes.\n\n    Returns\n    -------\n    None\n        The function writes the downloaded file to the specified location.\n\n    Raises\n    ------\n    Exception\n        If an error occurs during the download or file writing process.\n    \"\"\"\n    os.makedirs(save_path, exist_ok=True)\n    file_path = os.path.join(save_path, file_name)\n\n    try:\n        response = httpx.get(f\"{url}{file_name}\", follow_redirects=True)\n        with open(file_path, \"wb\") as file:\n            for chunk in response.iter_bytes(chunk_size=chunk_size):\n                file.write(chunk)\n    except Exception as e:\n        print(f\"Error during download: {e}\")\n    finally:\n        if response:\n            response.close()\n</code></pre>"},{"location":"app/tools/general/#app.tools.general.extract_zip","title":"<code>extract_zip(zip_path, zip_file, extract_to)</code>","text":"<p>Extract the contents of a zip file to the specified directory.</p>"},{"location":"app/tools/general/#app.tools.general.extract_zip--parameters","title":"Parameters","text":"<p>zip_path : str     The path to the zip file. zip_file : str     The name of the zip file. extract_to : str     The directory where the contents of the zip file will be extracted.</p> Source code in <code>app/tools/general.py</code> <pre><code>def extract_zip(\n    zip_path: str,\n    zip_file: str,\n    extract_to: str,\n) -&gt; None:\n    \"\"\"\n    Extract the contents of a zip file to the specified directory.\n\n    Parameters\n    ----------\n    zip_path : str\n        The path to the zip file.\n    zip_file : str\n        The name of the zip file.\n    extract_to : str\n        The directory where the contents of the zip file will be extracted.\n    \"\"\"\n    os.makedirs(extract_to, exist_ok=True)\n    file_path = os.path.join(zip_path, zip_file)\n\n    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n</code></pre>"},{"location":"app/tools/pipeline/","title":"Tools &gt; Pipeline","text":"<p>Data Collectors.</p> <p>This module provides full data pipeline.</p>"},{"location":"app/tools/pipeline/#app.tools.pipeline.run_pipeline","title":"<code>run_pipeline(years_to_process, inmet_url, save_path, stage_path, output_path, stations_file, weather_file, stations_column_names, weather_column_names, stations_schema, weather_schema)</code>","text":"<p>Execute the data collection pipeline for specified years.</p> <p>This function manages the workflow of downloading, extracting, processing, and saving weather station data. It iterates over each year provided, downloading and processing the relevant data files.</p>"},{"location":"app/tools/pipeline/#app.tools.pipeline.run_pipeline--parameters","title":"Parameters","text":"<p>years_to_process : List[int]     A list of years to process. inmet_url : str     The base URL from which the data files are downloaded. save_path : str     The local folder where downloaded zip files are saved. stage_path : str     The path to the folder where extracted data is temporarily stored. output_path : str     The path to the folder where processed data files are stored. stations_file : str     The name of the output file for processed data. stations_column_names : Dict[str, str]     Mapping of original column names to desired column names. schema     The Pydantic schema used for data validation.</p>"},{"location":"app/tools/pipeline/#app.tools.pipeline.run_pipeline--returns","title":"Returns","text":"<p>None     This function does not return anything. It processes and saves data     to the specified locations.</p>"},{"location":"app/tools/pipeline/#app.tools.pipeline.run_pipeline--raises","title":"Raises","text":"<p>Exception     If any step in the process (downloading, extracting, processing,     saving) fails, an exception is raised considering the functions     applied.</p> Source code in <code>app/tools/pipeline.py</code> <pre><code>@logger_decorator\ndef run_pipeline(\n    years_to_process: List[int],\n    inmet_url: str,\n    save_path: str,\n    stage_path: str,\n    output_path: str,\n    stations_file: str,\n    weather_file: str,\n    stations_column_names: Dict[str, str],\n    weather_column_names: Dict[str, str],\n    stations_schema,\n    weather_schema,\n) -&gt; None:\n    \"\"\"\n    Execute the data collection pipeline for specified years.\n\n    This function manages the workflow of downloading, extracting, processing,\n    and saving weather station data. It iterates over each year provided,\n    downloading and processing the relevant data files.\n\n    Parameters\n    ----------\n    years_to_process : List[int]\n        A list of years to process.\n    inmet_url : str\n        The base URL from which the data files are downloaded.\n    save_path : str\n        The local folder where downloaded zip files are saved.\n    stage_path : str\n        The path to the folder where extracted data is temporarily stored.\n    output_path : str\n        The path to the folder where processed data files are stored.\n    stations_file : str\n        The name of the output file for processed data.\n    stations_column_names : Dict[str, str]\n        Mapping of original column names to desired column names.\n    schema\n        The Pydantic schema used for data validation.\n\n    Returns\n    -------\n    None\n        This function does not return anything. It processes and saves data\n        to the specified locations.\n\n    Raises\n    ------\n    Exception\n        If any step in the process (downloading, extracting, processing,\n        saving) fails, an exception is raised considering the functions\n        applied.\n    \"\"\"\n    years = collect_years_list(years_to_process)\n    logger.info(f\"Start extraction data from years {years}\")\n\n    for year in years:\n        file_name = str(year) + \".zip\"\n\n        logger.info(f\"Beginning download data from {year}\")\n        download_file(inmet_url, file_name, save_path)\n\n        logger.info(f\"Beginning unzip data from {year}\")\n        extract_zip(save_path, file_name, stage_path)\n\n        logger.info(f\"Finishing process {year}\")\n\n    logger.info(\"Starting processing stations data\")\n    stations_data = StationDataCollector(\n        stage_path,\n        output_path,\n        stations_file,\n        stations_column_names,\n        stations_schema,\n    )\n    stations_data.start()\n\n    logger.info(\"Starting processing weather data\")\n    weather_data = WeatherDataCollector(\n        stage_path,\n        output_path,\n        weather_file,\n        weather_column_names,\n        weather_schema,\n    )\n    weather_data.start()\n\n    logger.info(\"Cleaning temp folders\")\n    clear_folder(save_path)\n    clear_folder(stage_path)\n\n    logger.info(\"Finish the pipeline!\")\n</code></pre>"},{"location":"app/tools/validators/","title":"Tools &gt; Validators","text":"<p>Data Validators.</p> <p>This module provides utility functions for data validation, such as asserting that all sublists within a list have the same elements, regardless of their order.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData","title":"<code>StationData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represent the weather station's data with validation rules, ensuring data consistency and integrity.</p> <p>This model includes comprehensive validation for each attribute to ensure that data about weather stations is accurate and in the correct format. It handles geographical coordinates, station identification, and operational dates with specific constraints.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData--attributes","title":"Attributes:","text":"<p>Region : str     The geographical region code of the weather station, automatically     converted to uppercase. It must be between 1 to 2 characters long. State : str     The state code where the weather station is located, automatically     converted to uppercase and required to be exactly 2 characters long. StationName : str     The name of the weather station, automatically converted to uppercase.     It can include both letters and numbers. IdStationWho : IdStationWhoType     A unique identifier for the weather station, following a specific     format ('A' followed by 3 digits). Latitude : float     The geographical latitude of the station. This model accepts both     comma and dot as decimal separators to accommodate different formats. Longitude : float     The geographical longitude of the station. Similar to Latitude,     it accepts both comma and dot for decimal separation. Altitude : float     The station's altitude in meters above sea level. Accepts string     input with comma or dot decimal separators and converts it to a float. FoundingDate : date     The date when the station was established. It supports various date     formats, including 'dd/mm/yyyy' and 'dd/mm/yy', and ensures that the     date is converted into a standard date object.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData--methods","title":"Methods:","text":"<p>parse_geo_coords(cls, value: str) -&gt; float:     Class method to parse geographical coordinates from string to float.     It's designed to accommodate the Brazilian format for decimal numbers,     converting commas to dots. parse_date(cls, value: str) -&gt; date:     Class method to parse and validate the founding date from a string     into a <code>datetime.date</code> object. It supports multiple date formats     for flexibility.</p> Source code in <code>app/tools/validators.py</code> <pre><code>class StationData(BaseModel):\n    \"\"\"Represent the weather station's data with validation rules, ensuring data consistency and integrity.\n\n    This model includes comprehensive validation for each attribute to ensure\n    that data about weather stations is accurate and in the correct format. It\n    handles geographical coordinates, station identification, and operational\n    dates with specific constraints.\n\n    Attributes:\n    ----------\n    Region : str\n        The geographical region code of the weather station, automatically\n        converted to uppercase. It must be between 1 to 2 characters long.\n    State : str\n        The state code where the weather station is located, automatically\n        converted to uppercase and required to be exactly 2 characters long.\n    StationName : str\n        The name of the weather station, automatically converted to uppercase.\n        It can include both letters and numbers.\n    IdStationWho : IdStationWhoType\n        A unique identifier for the weather station, following a specific\n        format ('A' followed by 3 digits).\n    Latitude : float\n        The geographical latitude of the station. This model accepts both\n        comma and dot as decimal separators to accommodate different formats.\n    Longitude : float\n        The geographical longitude of the station. Similar to Latitude,\n        it accepts both comma and dot for decimal separation.\n    Altitude : float\n        The station's altitude in meters above sea level. Accepts string\n        input with comma or dot decimal separators and converts it to a float.\n    FoundingDate : date\n        The date when the station was established. It supports various date\n        formats, including 'dd/mm/yyyy' and 'dd/mm/yy', and ensures that the\n        date is converted into a standard date object.\n\n    Methods:\n    --------\n    parse_geo_coords(cls, value: str) -&gt; float:\n        Class method to parse geographical coordinates from string to float.\n        It's designed to accommodate the Brazilian format for decimal numbers,\n        converting commas to dots.\n    parse_date(cls, value: str) -&gt; date:\n        Class method to parse and validate the founding date from a string\n        into a `datetime.date` object. It supports multiple date formats\n        for flexibility.\n    \"\"\"\n\n    Region: Annotated[\n        str,\n        StringConstraints(\n            min_length=1,\n            max_length=2,\n            to_upper=True,\n        ),\n    ]\n    State: Annotated[\n        str,\n        StringConstraints(\n            min_length=2,\n            max_length=2,\n            to_upper=True,\n        ),\n    ]\n    StationName: Annotated[\n        str,\n        StringConstraints(to_upper=True),\n    ]\n    IdStationWho: IdStationWhoType\n    Latitude: Any\n    Longitude: Any\n    Altitude: Any\n    FoundingDate: Any\n\n    @field_validator(\n        \"Latitude\",\n        \"Longitude\",\n        \"Altitude\",\n        mode=\"before\",\n    )\n    @classmethod\n    def parse_geo_coords(cls, value):\n        \"\"\"Parse a string input representing geographic coordinates, converting it to a float.\n\n        This method accommodates the common Brazilian format for decimal\n        numbers, where commas are used as decimal separators.\n\n        Parameters:\n        ----------\n        value : str\n            The geographic coordinate as a string, potentially using a comma\n            for decimal separation.\n\n        Returns:\n        -------\n        float\n            The geographic coordinate as a float.\n\n        Raises:\n        ------\n        ValueError\n            If the input string cannot be parsed into a float, indicating an\n            invalid format.\n        \"\"\"\n        try:\n            return float(value.replace(\",\", \".\"))\n        except ValueError:\n            raise ValueError(f\"Geographic Coordinate Invalid: {value}\")\n\n    @field_validator(\n        \"FoundingDate\",\n        mode=\"before\",\n    )\n    @classmethod\n    def parse_date(cls, value):\n        \"\"\"\n        Parse and validate foundation dates in multiple formats.\n\n        This validator function attempts to parse the date from a given\n        string. It supports two date formats: 'dd/mm/yyyy' and 'dd/mm/yy'.\n        This flexibility allows for handling variations in the date format.\n\n        Parameters\n        ----------\n        value : str\n            The string value of the date to be parsed.\n\n        Returns\n        -------\n        datetime.date\n            The parsed date as a datetime.date object.\n\n        Raises\n        ------\n        ValueError\n            If the provided value does not match any of the supported date\n            formats.\n\n        Example\n        -------\n        &gt;&gt;&gt; parse_date(\"19/07/2020\")\n        datetime.date(2020, 7, 19)\n        &gt;&gt;&gt; parse_date(\"19/07/20\")\n        datetime.date(2020, 7, 19)\n        \"\"\"\n        for date_format in (\"%d/%m/%Y\", \"%d/%m/%y\"):\n            try:\n                return datetime.strptime(value, date_format).date()\n            except ValueError:\n                pass\n        raise ValueError(f\"Foundation Date Invalid: {value}\")\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_date","title":"<code>parse_date(value)</code>  <code>classmethod</code>","text":"<p>Parse and validate foundation dates in multiple formats.</p> <p>This validator function attempts to parse the date from a given string. It supports two date formats: 'dd/mm/yyyy' and 'dd/mm/yy'. This flexibility allows for handling variations in the date format.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_date--parameters","title":"Parameters","text":"<p>value : str     The string value of the date to be parsed.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_date--returns","title":"Returns","text":"<p>datetime.date     The parsed date as a datetime.date object.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_date--raises","title":"Raises","text":"<p>ValueError     If the provided value does not match any of the supported date     formats.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_date--example","title":"Example","text":"<p>parse_date(\"19/07/2020\") datetime.date(2020, 7, 19) parse_date(\"19/07/20\") datetime.date(2020, 7, 19)</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"FoundingDate\",\n    mode=\"before\",\n)\n@classmethod\ndef parse_date(cls, value):\n    \"\"\"\n    Parse and validate foundation dates in multiple formats.\n\n    This validator function attempts to parse the date from a given\n    string. It supports two date formats: 'dd/mm/yyyy' and 'dd/mm/yy'.\n    This flexibility allows for handling variations in the date format.\n\n    Parameters\n    ----------\n    value : str\n        The string value of the date to be parsed.\n\n    Returns\n    -------\n    datetime.date\n        The parsed date as a datetime.date object.\n\n    Raises\n    ------\n    ValueError\n        If the provided value does not match any of the supported date\n        formats.\n\n    Example\n    -------\n    &gt;&gt;&gt; parse_date(\"19/07/2020\")\n    datetime.date(2020, 7, 19)\n    &gt;&gt;&gt; parse_date(\"19/07/20\")\n    datetime.date(2020, 7, 19)\n    \"\"\"\n    for date_format in (\"%d/%m/%Y\", \"%d/%m/%y\"):\n        try:\n            return datetime.strptime(value, date_format).date()\n        except ValueError:\n            pass\n    raise ValueError(f\"Foundation Date Invalid: {value}\")\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_geo_coords","title":"<code>parse_geo_coords(value)</code>  <code>classmethod</code>","text":"<p>Parse a string input representing geographic coordinates, converting it to a float.</p> <p>This method accommodates the common Brazilian format for decimal numbers, where commas are used as decimal separators.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_geo_coords--parameters","title":"Parameters:","text":"<p>value : str     The geographic coordinate as a string, potentially using a comma     for decimal separation.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_geo_coords--returns","title":"Returns:","text":"<p>float     The geographic coordinate as a float.</p>"},{"location":"app/tools/validators/#app.tools.validators.StationData.parse_geo_coords--raises","title":"Raises:","text":"<p>ValueError     If the input string cannot be parsed into a float, indicating an     invalid format.</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"Latitude\",\n    \"Longitude\",\n    \"Altitude\",\n    mode=\"before\",\n)\n@classmethod\ndef parse_geo_coords(cls, value):\n    \"\"\"Parse a string input representing geographic coordinates, converting it to a float.\n\n    This method accommodates the common Brazilian format for decimal\n    numbers, where commas are used as decimal separators.\n\n    Parameters:\n    ----------\n    value : str\n        The geographic coordinate as a string, potentially using a comma\n        for decimal separation.\n\n    Returns:\n    -------\n    float\n        The geographic coordinate as a float.\n\n    Raises:\n    ------\n    ValueError\n        If the input string cannot be parsed into a float, indicating an\n        invalid format.\n    \"\"\"\n    try:\n        return float(value.replace(\",\", \".\"))\n    except ValueError:\n        raise ValueError(f\"Geographic Coordinate Invalid: {value}\")\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData","title":"<code>WeatherData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represent meteorological data for a weather station, ensuring data integrity through validation.</p> <p>This model encapsulates and validates a range of meteorological measurements, such as temperature, humidity, atmospheric pressure, wind speed, and direction. It is designed to accommodate the nuances of meteorological data, including the allowance of NaN values for certain fields where data might be missing.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData--attributes","title":"Attributes:","text":"<p>IdStationWho : IdStationWhoType     The unique identifier for the weather station, adhering to a specific     format. Date : date     The date on which the meteorological measurements were taken. Time : time     The time at which the meteorological measurements were recorded, with     support for UTC notation. TotalPrecipitation : float     The total precipitation measured in millimeters. NaN values are     permitted to indicate missing or invalid data. MaxAtmosphericPressure : float     The maximum atmospheric pressure measured. NaN values are     permitted to indicate missing or invalid data. MinAtmosphericPressure : float     The minimum atmospheric pressure measured. NaN values are     permitted to indicate missing or invalid data. GlobalRadiation : float     The global radiation measured in Kj/m\u00b2. NaN values are     permitted to indicate missing or invalid data. DryBulbTemperature : float     The air temperature measured by a dry bulb thermometer in degrees     Celsius. NaN values are permitted to indicate missing or invalid data. DewPointTemperature : float     The dew point temperature in degrees Celsius.  NaN values are     permitted to indicate missing or invalid data. MaxTemperature : float     The maximum temperature recorded in the last hour in degrees Celsius.     NaN values are permitted to indicate missing or invalid data. MinTemperature : float     The minimum temperature recorded in the last hour in degrees Celsius.     NaN values are permitted to indicate missing or invalid data. MaxDewPointTemperature : float     The maximum dew point temperature recorded in the last hour in degrees     Celsius.  NaN values are permitted to indicate missing or invalid data. MinDewPointTemperature : float     The minimum dew point temperature recorded in the last hour in degrees     Celsius.  NaN values are permitted to indicate missing or invalid data. MaxRelativeHumidity : float     The maximum relative humidity recorded in the last hour, expressed as     a percentage.  NaN values are permitted to indicate missing or     invalid data. MinRelativeHumidity : float     The minimum relative humidity recorded in the last hour, expressed as     a percentage. Allows NaN values. RelativeHumidity : float     The relative humidity, expressed as a percentage. NaN values are     permitted to indicate missing or invalid data. WindDirection : float     The wind direction, in degrees from true north. NaN values are     permitted to indicate missing or invalid data. MaxWindGust : float     The maximum wind gust speed recorded in meters per second. NaN values     are permitted to indicate missing or invalid data. WindSpeed : float     The wind speed in meters per second. NaN values are     permitted to indicate missing or invalid data.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData--methods","title":"Methods:","text":"<p>parse_custom_date_format(value: str) -&gt; date:     Parses and validates a date string formatted as 'yyyy/mm/dd',     ensuring it conforms to this specific format. parse_time_utc(value: str) -&gt; time:     Parses and validates a time string formatted with UTC notation     ('HHMM UTC'), converting it to a <code>time</code> object. parse_to_float(value: float) -&gt; float | None:     Validates and adjusts float fields, specifically handling     NaN values. set_nan_out_range(value: float) -&gt; float | None:     Validates and adjusts float fields, specifically handling     NaN values and converting negative values to None.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData--notes","title":"Notes:","text":"<p>The inclusion of NaN values and the conversion of negative values to None are crucial for maintaining the integrity of meteorological data, acknowledging the presence of missing or non-applicable measurements.</p> Source code in <code>app/tools/validators.py</code> <pre><code>class WeatherData(BaseModel):\n    \"\"\"Represent meteorological data for a weather station, ensuring data integrity through validation.\n\n    This model encapsulates and validates a range of meteorological\n    measurements, such as temperature, humidity, atmospheric pressure, wind\n    speed, and direction. It is designed to accommodate the nuances\n    of meteorological data, including the allowance of NaN values for certain\n    fields where data might be missing.\n\n    Attributes:\n    -----------\n    IdStationWho : IdStationWhoType\n        The unique identifier for the weather station, adhering to a specific\n        format.\n    Date : date\n        The date on which the meteorological measurements were taken.\n    Time : time\n        The time at which the meteorological measurements were recorded, with\n        support for UTC notation.\n    TotalPrecipitation : float\n        The total precipitation measured in millimeters. NaN values are\n        permitted to indicate missing or invalid data.\n    MaxAtmosphericPressure : float\n        The maximum atmospheric pressure measured. NaN values are\n        permitted to indicate missing or invalid data.\n    MinAtmosphericPressure : float\n        The minimum atmospheric pressure measured. NaN values are\n        permitted to indicate missing or invalid data.\n    GlobalRadiation : float\n        The global radiation measured in Kj/m\u00b2. NaN values are\n        permitted to indicate missing or invalid data.\n    DryBulbTemperature : float\n        The air temperature measured by a dry bulb thermometer in degrees\n        Celsius. NaN values are permitted to indicate missing or invalid data.\n    DewPointTemperature : float\n        The dew point temperature in degrees Celsius.  NaN values are\n        permitted to indicate missing or invalid data.\n    MaxTemperature : float\n        The maximum temperature recorded in the last hour in degrees Celsius.\n        NaN values are permitted to indicate missing or invalid data.\n    MinTemperature : float\n        The minimum temperature recorded in the last hour in degrees Celsius.\n        NaN values are permitted to indicate missing or invalid data.\n    MaxDewPointTemperature : float\n        The maximum dew point temperature recorded in the last hour in degrees\n        Celsius.  NaN values are permitted to indicate missing or invalid data.\n    MinDewPointTemperature : float\n        The minimum dew point temperature recorded in the last hour in degrees\n        Celsius.  NaN values are permitted to indicate missing or invalid data.\n    MaxRelativeHumidity : float\n        The maximum relative humidity recorded in the last hour, expressed as\n        a percentage.  NaN values are permitted to indicate missing or\n        invalid data.\n    MinRelativeHumidity : float\n        The minimum relative humidity recorded in the last hour, expressed as\n        a percentage. Allows NaN values.\n    RelativeHumidity : float\n        The relative humidity, expressed as a percentage. NaN values are\n        permitted to indicate missing or invalid data.\n    WindDirection : float\n        The wind direction, in degrees from true north. NaN values are\n        permitted to indicate missing or invalid data.\n    MaxWindGust : float\n        The maximum wind gust speed recorded in meters per second. NaN values\n        are permitted to indicate missing or invalid data.\n    WindSpeed : float\n        The wind speed in meters per second. NaN values are\n        permitted to indicate missing or invalid data.\n\n    Methods:\n    --------\n    parse_custom_date_format(value: str) -&gt; date:\n        Parses and validates a date string formatted as 'yyyy/mm/dd',\n        ensuring it conforms to this specific format.\n    parse_time_utc(value: str) -&gt; time:\n        Parses and validates a time string formatted with UTC notation\n        ('HHMM UTC'), converting it to a `time` object.\n    parse_to_float(value: float) -&gt; float | None:\n        Validates and adjusts float fields, specifically handling\n        NaN values.\n    set_nan_out_range(value: float) -&gt; float | None:\n        Validates and adjusts float fields, specifically handling\n        NaN values and converting negative values to None.\n\n    Notes:\n    ------\n    The inclusion of NaN values and the conversion of negative\n    values to None are crucial for maintaining the integrity of\n    meteorological data, acknowledging the presence of missing\n    or non-applicable measurements.\n    \"\"\"\n\n    IdStationWho: IdStationWhoType\n    Date: date\n    Time: time\n    TotalPrecipitation: Annotated[Any, Field(default=None)]\n    MaxAtmosphericPressure: Annotated[Any, Field(default=None)]\n    MinAtmosphericPressure: Annotated[Any, Field(default=None)]\n    GlobalRadiation: Annotated[Any, Field(default=None)]\n    DryBulbTemperature: Annotated[Any, Field(default=None)]\n    DewPointTemperature: Annotated[Any, Field(default=None)]\n    MaxTemperature: Annotated[Any, Field(default=None)]\n    MinTemperature: Annotated[Any, Field(default=None)]\n    MaxDewPointTemperature: Annotated[Any, Field(default=None)]\n    MinDewPointTemperature: Annotated[Any, Field(default=None)]\n    MaxRelativeHumidity: Annotated[Any, Field(default=None)]\n    MinRelativeHumidity: Annotated[Any, Field(default=None)]\n    RelativeHumidity: Annotated[Any, Field(default=None)]\n    WindDirection: Annotated[Any, Field(default=None)]\n    MaxWindGust: Annotated[Any, Field(default=None)]\n    WindSpeed: Annotated[Any, Field(default=None)]\n\n    @field_validator(\n        \"Date\",\n        mode=\"before\",\n    )\n    @classmethod\n    def parse_custom_date_format(\n        cls,\n        value,\n    ):\n        \"\"\"Parse and validates a string representing a date into a date object, expecting the format 'yyyy/mm/dd'.\n\n        This method is designed to ensure consistency in date representation\n        within meteorological data, specifically accommodating the\n        international standard format.\n\n        Parameters:\n        ----------\n        value : str\n            The string representation of a date, expected to be in the\n            'yyyy/mm/dd' format.\n\n        Returns:\n        -------\n        datetime.date\n            The date converted into a date object.\n\n        Raises:\n        ------\n        ValueError\n            If the input string does not match the expected date format,\n            indicating an invalid date format.\n        \"\"\"\n        if isinstance(value, str):\n            try:\n                return datetime.strptime(value, \"%Y/%m/%d\").date()\n            except ValueError:\n                raise ValueError(\n                    f\"Invalid date format for {value}, expected yyyy/mm/dd\",\n                )\n        return value\n\n    @field_validator(\n        \"Time\",\n        mode=\"before\",\n    )\n    @classmethod\n    def parse_time_utc(\n        cls,\n        value,\n    ):\n        \"\"\"Parse a string representing time with UTC notation ('HHMM UTC') into a time object.\n\n        This function standardizes the representation of time within the\n        dataset, aligning with international time notation standards.\n\n        Parameters:\n        ----------\n        value : str\n            The time as a string in 'HHMM UTC' format.\n\n        Returns:\n        -------\n        datetime.time\n            The time converted into a time object.\n\n        Raises:\n        ------\n        ValueError\n            If the string is not in the 'HHMM UTC' format or cannot be parsed\n            into a time object.\n        \"\"\"\n        if value and \"UTC\" in value:\n            hour = int(value.split(\" \")[0][:2])\n            minute = int(value.split(\" \")[0][2:])\n            return time(hour, minute)\n        raise ValueError(\"Invalid time format.\")\n\n    @field_validator(\n        \"DryBulbTemperature\",\n        \"DewPointTemperature\",\n        \"MaxTemperature\",\n        \"MinTemperature\",\n        \"MaxDewPointTemperature\",\n        \"MinDewPointTemperature\",\n        mode=\"before\",\n    )\n    @classmethod\n    def parse_to_float(\n        cls,\n        value,\n    ):\n        \"\"\"Parse and validates string inputs for temperature and dew point fields, allowing for Brazilian numeric format.\n\n        Converts string representations of numerical values, which may use\n        commas as decimal separators, into floats. This caters to the\n        Brazilian format for decimal numbers and ensures that the data\n        is accurately represented and validated.\n\n        Parameters:\n        ----------\n        value : str\n            The string representation of a numerical value, potentially\n            using a comma as the decimal separator.\n\n        Returns:\n        -------\n        float\n            The numeric value converted into a float.\n\n        Raises:\n        ------\n        ValueError\n            If the input value cannot be converted into a float,\n            indicating an invalid numeric format.\n        \"\"\"\n        try:\n            if value:\n                return float(str(value).replace(\",\", \".\"))\n            else:\n                return None\n        except ValueError:\n            return None\n\n    @field_validator(\n        \"TotalPrecipitation\",\n        \"MaxAtmosphericPressure\",\n        \"MinAtmosphericPressure\",\n        \"MaxRelativeHumidity\",\n        \"MinRelativeHumidity\",\n        \"RelativeHumidity\",\n        \"WindDirection\",\n        \"MaxWindGust\",\n        \"WindSpeed\",\n        \"GlobalRadiation\",\n        mode=\"before\",\n    )\n    @classmethod\n    def set_nan_out_range(\n        cls,\n        value,\n    ):\n        \"\"\"Validate numerical fields, allowing NaN values and converting negative or improperly formatted values to None.\n\n        This method ensures that meteorological measurements are within logical\n        ranges, acknowledging the possibility of missing data (represented as\n        NaN) and correcting any negative values that do not make sense in the\n        context of the measurement being taken.\n\n        Parameters:\n        ----------\n        value : Any\n            The value to validate, which may be a numerical value or NaN.\n\n        Returns:\n        -------\n        float | None\n            The original value if it's a valid number or None if the value\n            is negative or improperly formatted.\n\n        Note:\n        ----\n        This method emphasizes the flexibility required in handling\n        meteorological data, particularly in accommodating missing data\n        points and ensuring data integrity.\n        \"\"\"\n        if value:\n            float_parsed = float(str(value).replace(\",\", \".\"))\n            if float_parsed &gt;= 0:\n                return float_parsed\n        return None\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_custom_date_format","title":"<code>parse_custom_date_format(value)</code>  <code>classmethod</code>","text":"<p>Parse and validates a string representing a date into a date object, expecting the format 'yyyy/mm/dd'.</p> <p>This method is designed to ensure consistency in date representation within meteorological data, specifically accommodating the international standard format.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_custom_date_format--parameters","title":"Parameters:","text":"<p>value : str     The string representation of a date, expected to be in the     'yyyy/mm/dd' format.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_custom_date_format--returns","title":"Returns:","text":"<p>datetime.date     The date converted into a date object.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_custom_date_format--raises","title":"Raises:","text":"<p>ValueError     If the input string does not match the expected date format,     indicating an invalid date format.</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"Date\",\n    mode=\"before\",\n)\n@classmethod\ndef parse_custom_date_format(\n    cls,\n    value,\n):\n    \"\"\"Parse and validates a string representing a date into a date object, expecting the format 'yyyy/mm/dd'.\n\n    This method is designed to ensure consistency in date representation\n    within meteorological data, specifically accommodating the\n    international standard format.\n\n    Parameters:\n    ----------\n    value : str\n        The string representation of a date, expected to be in the\n        'yyyy/mm/dd' format.\n\n    Returns:\n    -------\n    datetime.date\n        The date converted into a date object.\n\n    Raises:\n    ------\n    ValueError\n        If the input string does not match the expected date format,\n        indicating an invalid date format.\n    \"\"\"\n    if isinstance(value, str):\n        try:\n            return datetime.strptime(value, \"%Y/%m/%d\").date()\n        except ValueError:\n            raise ValueError(\n                f\"Invalid date format for {value}, expected yyyy/mm/dd\",\n            )\n    return value\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_time_utc","title":"<code>parse_time_utc(value)</code>  <code>classmethod</code>","text":"<p>Parse a string representing time with UTC notation ('HHMM UTC') into a time object.</p> <p>This function standardizes the representation of time within the dataset, aligning with international time notation standards.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_time_utc--parameters","title":"Parameters:","text":"<p>value : str     The time as a string in 'HHMM UTC' format.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_time_utc--returns","title":"Returns:","text":"<p>datetime.time     The time converted into a time object.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_time_utc--raises","title":"Raises:","text":"<p>ValueError     If the string is not in the 'HHMM UTC' format or cannot be parsed     into a time object.</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"Time\",\n    mode=\"before\",\n)\n@classmethod\ndef parse_time_utc(\n    cls,\n    value,\n):\n    \"\"\"Parse a string representing time with UTC notation ('HHMM UTC') into a time object.\n\n    This function standardizes the representation of time within the\n    dataset, aligning with international time notation standards.\n\n    Parameters:\n    ----------\n    value : str\n        The time as a string in 'HHMM UTC' format.\n\n    Returns:\n    -------\n    datetime.time\n        The time converted into a time object.\n\n    Raises:\n    ------\n    ValueError\n        If the string is not in the 'HHMM UTC' format or cannot be parsed\n        into a time object.\n    \"\"\"\n    if value and \"UTC\" in value:\n        hour = int(value.split(\" \")[0][:2])\n        minute = int(value.split(\" \")[0][2:])\n        return time(hour, minute)\n    raise ValueError(\"Invalid time format.\")\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_to_float","title":"<code>parse_to_float(value)</code>  <code>classmethod</code>","text":"<p>Parse and validates string inputs for temperature and dew point fields, allowing for Brazilian numeric format.</p> <p>Converts string representations of numerical values, which may use commas as decimal separators, into floats. This caters to the Brazilian format for decimal numbers and ensures that the data is accurately represented and validated.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_to_float--parameters","title":"Parameters:","text":"<p>value : str     The string representation of a numerical value, potentially     using a comma as the decimal separator.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_to_float--returns","title":"Returns:","text":"<p>float     The numeric value converted into a float.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.parse_to_float--raises","title":"Raises:","text":"<p>ValueError     If the input value cannot be converted into a float,     indicating an invalid numeric format.</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"DryBulbTemperature\",\n    \"DewPointTemperature\",\n    \"MaxTemperature\",\n    \"MinTemperature\",\n    \"MaxDewPointTemperature\",\n    \"MinDewPointTemperature\",\n    mode=\"before\",\n)\n@classmethod\ndef parse_to_float(\n    cls,\n    value,\n):\n    \"\"\"Parse and validates string inputs for temperature and dew point fields, allowing for Brazilian numeric format.\n\n    Converts string representations of numerical values, which may use\n    commas as decimal separators, into floats. This caters to the\n    Brazilian format for decimal numbers and ensures that the data\n    is accurately represented and validated.\n\n    Parameters:\n    ----------\n    value : str\n        The string representation of a numerical value, potentially\n        using a comma as the decimal separator.\n\n    Returns:\n    -------\n    float\n        The numeric value converted into a float.\n\n    Raises:\n    ------\n    ValueError\n        If the input value cannot be converted into a float,\n        indicating an invalid numeric format.\n    \"\"\"\n    try:\n        if value:\n            return float(str(value).replace(\",\", \".\"))\n        else:\n            return None\n    except ValueError:\n        return None\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.set_nan_out_range","title":"<code>set_nan_out_range(value)</code>  <code>classmethod</code>","text":"<p>Validate numerical fields, allowing NaN values and converting negative or improperly formatted values to None.</p> <p>This method ensures that meteorological measurements are within logical ranges, acknowledging the possibility of missing data (represented as NaN) and correcting any negative values that do not make sense in the context of the measurement being taken.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.set_nan_out_range--parameters","title":"Parameters:","text":"<p>value : Any     The value to validate, which may be a numerical value or NaN.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.set_nan_out_range--returns","title":"Returns:","text":"<p>float | None     The original value if it's a valid number or None if the value     is negative or improperly formatted.</p>"},{"location":"app/tools/validators/#app.tools.validators.WeatherData.set_nan_out_range--note","title":"Note:","text":"<p>This method emphasizes the flexibility required in handling meteorological data, particularly in accommodating missing data points and ensuring data integrity.</p> Source code in <code>app/tools/validators.py</code> <pre><code>@field_validator(\n    \"TotalPrecipitation\",\n    \"MaxAtmosphericPressure\",\n    \"MinAtmosphericPressure\",\n    \"MaxRelativeHumidity\",\n    \"MinRelativeHumidity\",\n    \"RelativeHumidity\",\n    \"WindDirection\",\n    \"MaxWindGust\",\n    \"WindSpeed\",\n    \"GlobalRadiation\",\n    mode=\"before\",\n)\n@classmethod\ndef set_nan_out_range(\n    cls,\n    value,\n):\n    \"\"\"Validate numerical fields, allowing NaN values and converting negative or improperly formatted values to None.\n\n    This method ensures that meteorological measurements are within logical\n    ranges, acknowledging the possibility of missing data (represented as\n    NaN) and correcting any negative values that do not make sense in the\n    context of the measurement being taken.\n\n    Parameters:\n    ----------\n    value : Any\n        The value to validate, which may be a numerical value or NaN.\n\n    Returns:\n    -------\n    float | None\n        The original value if it's a valid number or None if the value\n        is negative or improperly formatted.\n\n    Note:\n    ----\n    This method emphasizes the flexibility required in handling\n    meteorological data, particularly in accommodating missing data\n    points and ensuring data integrity.\n    \"\"\"\n    if value:\n        float_parsed = float(str(value).replace(\",\", \".\"))\n        if float_parsed &gt;= 0:\n            return float_parsed\n    return None\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.validate_data_quality","title":"<code>validate_data_quality(df, output_path, file_name, schema)</code>","text":"<p>Validate each row in a DataFrame against a Pydantic schema and log validation errors.</p> <p>This function iterates through the DataFrame, attempting to create instances of the specified Pydantic schema with each row's data. If a row fails validation, the error is logged. The log file is created only if there are invalid records.</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_data_quality--parameters","title":"Parameters:","text":"<p>df : pd.DataFrame     The DataFrame containing data to be validated. output_path : str     The directory path where the log file will be saved, if necessary. file_name : str     The name of the log file for recording validation errors, without     the extension. schema : BaseModel     The Pydantic model against which data rows will be validated.</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_data_quality--yields","title":"Yields:","text":"<p>Yields instances of the Pydantic schema for valid rows or logs validation errors for invalid rows.</p> Source code in <code>app/tools/validators.py</code> <pre><code>def validate_data_quality(\n    df: pd.DataFrame,\n    output_path: str,\n    file_name: str,\n    schema: BaseModel,\n):\n    \"\"\"Validate each row in a DataFrame against a Pydantic schema and log validation errors.\n\n    This function iterates through the DataFrame, attempting to create\n    instances of the specified Pydantic schema with each row's data.\n    If a row fails validation, the error is logged. The log file is\n    created only if there are invalid records.\n\n    Parameters:\n    ----------\n    df : pd.DataFrame\n        The DataFrame containing data to be validated.\n    output_path : str\n        The directory path where the log file will be saved, if necessary.\n    file_name : str\n        The name of the log file for recording validation errors, without\n        the extension.\n    schema : BaseModel\n        The Pydantic model against which data rows will be validated.\n\n    Yields:\n    ------\n    Yields instances of the Pydantic schema for valid rows or logs validation\n    errors for invalid rows.\n    \"\"\"\n    error_messages = []\n    for index, row in df.iterrows():\n        try:\n            yield schema(**row.to_dict()).model_dump()\n        except ValidationError as e:\n            error_message = f\"{index}: {e.json()}\\n\"\n            error_messages.append(error_message)\n\n    if error_messages:\n        log_path = os.path.join(\n            output_path,\n            f\"{file_name}_invalid_records.log\",\n        )\n        with open(log_path, \"w\") as file:\n            file.writelines(error_messages)\n</code></pre>"},{"location":"app/tools/validators/#app.tools.validators.validate_sublists","title":"<code>validate_sublists(list_with_sublists)</code>","text":"<p>Confirm if all sublists within a given list contain identical elements, regardless of their order.</p> <p>This function is crucial for ensuring dataframes have consistent column names across multiple files. It assesses each sublist (representing dataframe columns) to verify they all contain the same elements (column names).</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_sublists--parameters","title":"Parameters:","text":"<p>list_with_sublists : List[List[str]]     A list containing sublists to be validated for identical elements.</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_sublists--returns","title":"Returns:","text":"<p>bool     True if all sublists contain identical elements; False otherwise.</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_sublists--raises","title":"Raises:","text":"<p>ValueError     Raised if any sublist differs in elements, indicating inconsistent     column names.</p>"},{"location":"app/tools/validators/#app.tools.validators.validate_sublists--examples","title":"Examples:","text":"<p>validate_sublists([[\"A\", \"B\"], [\"B\", \"A\"]]) True</p> <p>validate_sublists([[\"C\", \"B\"], [\"B\", \"A\"]]) ValueError: Sublists do not contain the same elements.</p> Source code in <code>app/tools/validators.py</code> <pre><code>def validate_sublists(list_with_sublists: List[List[str]]) -&gt; bool:\n    \"\"\"Confirm if all sublists within a given list contain identical elements, regardless of their order.\n\n    This function is crucial for ensuring dataframes have consistent column\n    names across multiple files. It assesses each sublist (representing\n    dataframe columns) to verify they all contain the same elements\n    (column names).\n\n    Parameters:\n    ----------\n    list_with_sublists : List[List[str]]\n        A list containing sublists to be validated for identical elements.\n\n    Returns:\n    -------\n    bool\n        True if all sublists contain identical elements; False otherwise.\n\n    Raises:\n    ------\n    ValueError\n        Raised if any sublist differs in elements, indicating inconsistent\n        column names.\n\n    Examples:\n    --------\n    &gt;&gt;&gt; validate_sublists([[\"A\", \"B\"], [\"B\", \"A\"]])\n    True\n\n    &gt;&gt;&gt; validate_sublists([[\"C\", \"B\"], [\"B\", \"A\"]])\n    ValueError: Sublists do not contain the same elements.\n    \"\"\"\n    set_columns = set(tuple(sorted(sublist)) for sublist in list_with_sublists)\n    if len(set_columns) == 1:\n        return True\n    else:\n        raise ValueError(\"Sublists do not have the same elements.\")\n</code></pre>"},{"location":"tests/test_api/","title":"Test API","text":""},{"location":"tests/test_api/#tests.test_api.TestStationsRoute","title":"<code>TestStationsRoute</code>","text":"<p>Run tests on stations routes.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute--notes","title":"Notes","text":"<p>These tests could be done using test parameters to avoid repetition.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute--asserts","title":"Asserts","text":"<p>The '/' route should return a status code of 200, and redirect user to the API documentation.</p> Source code in <code>tests/test_api.py</code> <pre><code>class TestStationsRoute:\n    \"\"\"Run tests on stations routes.\n\n    Notes\n    -------\n    These tests could be done using test parameters to avoid repetition.\n\n    Asserts\n    -------\n    The '/' route should return a status code of 200, and redirect user to\n    the API documentation.\n    \"\"\"\n\n    def test_stations_route(\n        self,\n        test_client,\n    ):\n        \"\"\"Test the '/stations' route for correct response structure and data.\n\n        Parameters\n        ----------\n        test_client : TestClient\n            An instance of TestClient for making requests to the FastAPI app.\n\n        Asserts\n        -------\n        The '/stations' route should return a status code of 200, the response\n        should be a list, and the data should match the expected column names\n        and have more than one row.\n        \"\"\"\n        response = test_client.get(\"/stations\")\n\n        response_json = response.json()\n\n        response_df = pd.DataFrame(response_json)\n\n        response_number_rows, _ = response_df.shape\n\n        assert response.status_code == 200\n        assert isinstance(response_json, list)\n        assert response_df.columns.to_list() == [\n            column for column in STATION_COLUMN_NAMES.values()\n        ]\n        assert response_number_rows &gt; 1\n\n    def test_station_route(\n        self,\n        test_client,\n    ):\n        \"\"\"Test the '/stations/{IdStationWho}' route for correct response\n        structure and data.\n\n        Parameters\n        ----------\n        test_client : TestClient\n            An instance of TestClient for making requests to the FastAPI app.\n\n        Asserts\n        -------\n        The '/stations/{IdStationWho}' route should return a status code of\n        200, the response should be a list, and the data should match the\n        expected column names, contain exactly one row, and the 'IdStationWho'\n        column should match the requested station ID.\n        \"\"\"\n        response = test_client.get(\"/stations/A721\")\n\n        response_json = response.json()\n\n        response_df = pd.DataFrame(response_json)\n\n        response_number_rows, _ = response_df.shape\n\n        assert response.status_code == 200\n        assert isinstance(response_json, list)\n        assert response_df.columns.to_list() == [\n            column for column in STATION_COLUMN_NAMES.values()\n        ]\n        assert response_df[\"IdStationWho\"][0] == \"A721\"\n        assert response_number_rows == 1\n\n    def test_station_invalid(\n        self,\n        test_client,\n    ):\n        \"\"\"Test the '/stations/{IdStationWho}' route for an incorrect\n        response.\n\n        Parameters\n        ----------\n        test_client : TestClient\n            An instance of TestClient for making requests to the FastAPI app.\n\n        Asserts\n        -------\n        The '/stations/{IdStationWho}' route should return a status code of\n        404 and deliver a message 'Station do not exist.' .\n        \"\"\"\n        response = test_client.get(\"/stations/A000\")\n\n        response_json = response.json()\n\n        assert response.status_code == 404\n        assert response_json == {\"detail\": \"Station do not exist.\"}\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_invalid","title":"<code>test_station_invalid(test_client)</code>","text":"<p>Test the '/stations/{IdStationWho}' route for an incorrect response.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_invalid--parameters","title":"Parameters","text":"<p>test_client : TestClient     An instance of TestClient for making requests to the FastAPI app.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_invalid--asserts","title":"Asserts","text":"<p>The '/stations/{IdStationWho}' route should return a status code of 404 and deliver a message 'Station do not exist.' .</p> Source code in <code>tests/test_api.py</code> <pre><code>def test_station_invalid(\n    self,\n    test_client,\n):\n    \"\"\"Test the '/stations/{IdStationWho}' route for an incorrect\n    response.\n\n    Parameters\n    ----------\n    test_client : TestClient\n        An instance of TestClient for making requests to the FastAPI app.\n\n    Asserts\n    -------\n    The '/stations/{IdStationWho}' route should return a status code of\n    404 and deliver a message 'Station do not exist.' .\n    \"\"\"\n    response = test_client.get(\"/stations/A000\")\n\n    response_json = response.json()\n\n    assert response.status_code == 404\n    assert response_json == {\"detail\": \"Station do not exist.\"}\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_route","title":"<code>test_station_route(test_client)</code>","text":"<p>Test the '/stations/{IdStationWho}' route for correct response structure and data.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_route--parameters","title":"Parameters","text":"<p>test_client : TestClient     An instance of TestClient for making requests to the FastAPI app.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_station_route--asserts","title":"Asserts","text":"<p>The '/stations/{IdStationWho}' route should return a status code of 200, the response should be a list, and the data should match the expected column names, contain exactly one row, and the 'IdStationWho' column should match the requested station ID.</p> Source code in <code>tests/test_api.py</code> <pre><code>def test_station_route(\n    self,\n    test_client,\n):\n    \"\"\"Test the '/stations/{IdStationWho}' route for correct response\n    structure and data.\n\n    Parameters\n    ----------\n    test_client : TestClient\n        An instance of TestClient for making requests to the FastAPI app.\n\n    Asserts\n    -------\n    The '/stations/{IdStationWho}' route should return a status code of\n    200, the response should be a list, and the data should match the\n    expected column names, contain exactly one row, and the 'IdStationWho'\n    column should match the requested station ID.\n    \"\"\"\n    response = test_client.get(\"/stations/A721\")\n\n    response_json = response.json()\n\n    response_df = pd.DataFrame(response_json)\n\n    response_number_rows, _ = response_df.shape\n\n    assert response.status_code == 200\n    assert isinstance(response_json, list)\n    assert response_df.columns.to_list() == [\n        column for column in STATION_COLUMN_NAMES.values()\n    ]\n    assert response_df[\"IdStationWho\"][0] == \"A721\"\n    assert response_number_rows == 1\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_stations_route","title":"<code>test_stations_route(test_client)</code>","text":"<p>Test the '/stations' route for correct response structure and data.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_stations_route--parameters","title":"Parameters","text":"<p>test_client : TestClient     An instance of TestClient for making requests to the FastAPI app.</p>"},{"location":"tests/test_api/#tests.test_api.TestStationsRoute.test_stations_route--asserts","title":"Asserts","text":"<p>The '/stations' route should return a status code of 200, the response should be a list, and the data should match the expected column names and have more than one row.</p> Source code in <code>tests/test_api.py</code> <pre><code>def test_stations_route(\n    self,\n    test_client,\n):\n    \"\"\"Test the '/stations' route for correct response structure and data.\n\n    Parameters\n    ----------\n    test_client : TestClient\n        An instance of TestClient for making requests to the FastAPI app.\n\n    Asserts\n    -------\n    The '/stations' route should return a status code of 200, the response\n    should be a list, and the data should match the expected column names\n    and have more than one row.\n    \"\"\"\n    response = test_client.get(\"/stations\")\n\n    response_json = response.json()\n\n    response_df = pd.DataFrame(response_json)\n\n    response_number_rows, _ = response_df.shape\n\n    assert response.status_code == 200\n    assert isinstance(response_json, list)\n    assert response_df.columns.to_list() == [\n        column for column in STATION_COLUMN_NAMES.values()\n    ]\n    assert response_number_rows &gt; 1\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.TestWeatherRoute","title":"<code>TestWeatherRoute</code>","text":"<p>Run tests on weather routes.</p>"},{"location":"tests/test_api/#tests.test_api.TestWeatherRoute--notes","title":"Notes","text":"<p>These tests could be done using test parameters to avoid repetition.</p> Source code in <code>tests/test_api.py</code> <pre><code>class TestWeatherRoute:\n    \"\"\"Run tests on weather routes.\n\n    Notes\n    -------\n    These tests could be done using test parameters to avoid repetition.\n    \"\"\"\n\n    def test_weather_parquet_integrity(self):\n        weather_parquet = f\"{WEATHER_FILE}.parquet\"\n        weather_db = os.path.join(OUTPUT_PATH, weather_parquet)\n        with open(weather_db, \"rb\") as f:\n            f.seek(-4, 2)\n            magic_bytes = f.read()\n            print(magic_bytes)\n            assert magic_bytes == b\"PAR1\"\n\n    def test_get_correct_data(\n        self,\n        test_client,\n    ):\n        response = test_client.get(\"/weather/A721/2023-01-01/2023-01-02/\")\n\n        response_json = response.json()\n\n        response_df = pd.DataFrame(response_json)\n\n        response_df_columns = response_df.columns.to_list()\n\n        assert response.status_code == 200\n        assert isinstance(response_json, list)\n        assert response_df_columns.sort() == WEATHER_DF_COLUMN_NAMES.sort()\n\n    def test_get_more_than_5_weeks(\n        self,\n        test_client,\n    ):\n        response = test_client.get(\"/weather/A721/2023-01-01/2023-05-02/\")\n        response_json = response.json()\n\n        assert response.status_code == 422\n        assert response_json == {\n            \"detail\": \"Maximum period between start_date and end_date should be 5 weeks.\",  # noqa\n        }  # noqa\n\n    def test_get_end_before_start(\n        self,\n        test_client,\n    ):\n        response = test_client.get(\"/weather/A721/2023-02-01/2023-01-02/\")\n        response_json = response.json()\n\n        assert response.status_code == 422\n        assert response_json == {\n            \"detail\": \"end_date should be after start_date.\",\n        }  # noqa\n\n    def test_get_future_data(\n        self,\n        test_client,\n    ):\n        response = test_client.get(\"/weather/A721/2099-01-01/2099-01-02/\")\n\n        assert response.status_code == 422\n\n    def test_empty_query(\n        self,\n        test_client,\n    ):\n        response = test_client.get(\"/weather/A721/1999-01-01/1999-01-02/\")\n\n        response_json = response.json()\n\n        assert response.status_code == 422\n        assert response_json == {\n            \"detail\": \"There is no data to show. Rewrite your query.\",\n        }\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.test_client","title":"<code>test_client()</code>","text":"<p>Create a test client for the FastAPI application.</p>"},{"location":"tests/test_api/#tests.test_api.test_client--yields","title":"Yields","text":"<p>TestClient     An instance of TestClient wrapped around the FastAPI app.</p> Source code in <code>tests/test_api.py</code> <pre><code>@pytest.fixture\ndef test_client():\n    \"\"\"Create a test client for the FastAPI application.\n\n    Yields\n    -------\n    TestClient\n        An instance of TestClient wrapped around the FastAPI app.\n    \"\"\"\n    with TestClient(app) as client:\n        yield client\n</code></pre>"},{"location":"tests/test_api/#tests.test_api.test_main_route","title":"<code>test_main_route(test_client)</code>","text":"<p>Run tests on root route.</p>"},{"location":"tests/test_api/#tests.test_api.test_main_route--parameters","title":"Parameters","text":"<p>test_client : TestClient     An instance of TestClient for making requests to the FastAPI app.</p> Source code in <code>tests/test_api.py</code> <pre><code>def test_main_route(test_client):\n    \"\"\"Run tests on root route.\n\n\n    Parameters\n    ----------\n    test_client : TestClient\n        An instance of TestClient for making requests to the FastAPI app.\n\n\n    \"\"\"\n    response = test_client.get(\"/\")\n    assert response.status_code == 200\n    assert response.url == \"http://testserver/docs\"\n</code></pre>"},{"location":"tests/tools/test_collectors/","title":"Test Tools &gt; Collectors","text":""},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList","title":"<code>TestCollectYearsList</code>","text":"Source code in <code>tests/tools/test_collectors.py</code> <pre><code>class TestCollectYearsList:\n    def test_empty_list(self):\n        \"\"\"Test the behavior of collect_years_list when provided with an\n        empty list.\n\n        An empty list should trigger a ValueError to indicate that valid years\n        are required for processing.\n\n        Raises\n        ------\n        ValueError\n            If the provided list is empty.\n        \"\"\"\n        empty_list = []  # type: ignore\n        with pytest.raises(ValueError) as excinfo:\n            collect_years_list(empty_list)\n        assert (\n            f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n            in str(excinfo.value)\n        )\n\n    def test_all_valid_years(\n        self,\n        list_with_valid_years,\n    ):\n        \"\"\"Test collect_years_list with a list containing only valid years.\n\n        The function should return the same list of years if all are valid\n        within the accepted range.\n\n        Parameters\n        ----------\n        list_with_valid_years : list\n            A list of years that are all valid and within the collection range.\n        \"\"\"\n        result_list = collect_years_list(list_with_valid_years)\n\n        assert result_list == [2000, 2010, 2023]\n\n    def test_all_invalid_years(\n        self,\n        list_with_invalid_int_years,\n        list_with_invalid_str_years,\n    ):\n        \"\"\"Test collect_years_list with lists that contain only invalid years.\n\n        The function is expected to raise a ValueError indicating that no\n        valid years were provided.\n\n        Parameters\n        ----------\n        list_with_invalid_int_years : list\n            A list of integers representing years, all of which are invalid.\n        list_with_invalid_str_years : list\n            A list of strings, none of which are valid representations of\n            years.\n\n        Raises\n        ------\n        ValueError\n            If the list does not contain any valid years.\n        \"\"\"\n        with pytest.raises(ValueError) as excinfo:\n            collect_years_list(list_with_invalid_int_years)\n        assert (\n            f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n            in str(excinfo.value)\n        )\n\n        with pytest.raises(ValueError) as excinfo:\n            collect_years_list(list_with_invalid_str_years)\n        assert (\n            f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n            in str(excinfo.value)\n        )\n\n    def test_mixed_list(\n        self,\n        list_with_mix_int_years,\n        list_with_mix_int_str_years,\n        capsys,\n    ):\n        \"\"\"Test collect_years_list with lists that contain a mixture of valid\n        and invalid years or strings.\n\n        Valid years should be returned and invalid ones should be reported via\n        standard output.\n\n        Parameters\n        ----------\n        list_with_mix_int_years : list\n            A list containing a mix of valid and invalid integer years.\n        list_with_mix_int_str_years : list\n            A list containing both valid integer years and invalid string\n            years.\n        capsys : fixture\n            Pytest fixture that captures standard output and error streams.\n        \"\"\"\n        result_with_int = collect_years_list(list_with_mix_int_years)\n\n        assert result_with_int == [2010, 2023]\n\n        assert (\n            capsys.readouterr().out\n            == \"The elements [1999] were removed from the list.\\n\"\n        )\n\n        result_with_str = collect_years_list(list_with_mix_int_str_years)\n\n        assert result_with_str == [2010, 2023]\n\n        assert (\n            capsys.readouterr().out\n            == \"The elements ['A'] were removed from the list.\\n\"\n        )\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_all_invalid_years","title":"<code>test_all_invalid_years(list_with_invalid_int_years, list_with_invalid_str_years)</code>","text":"<p>Test collect_years_list with lists that contain only invalid years.</p> <p>The function is expected to raise a ValueError indicating that no valid years were provided.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_all_invalid_years--parameters","title":"Parameters","text":"<p>list_with_invalid_int_years : list     A list of integers representing years, all of which are invalid. list_with_invalid_str_years : list     A list of strings, none of which are valid representations of     years.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_all_invalid_years--raises","title":"Raises","text":"<p>ValueError     If the list does not contain any valid years.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_all_invalid_years(\n    self,\n    list_with_invalid_int_years,\n    list_with_invalid_str_years,\n):\n    \"\"\"Test collect_years_list with lists that contain only invalid years.\n\n    The function is expected to raise a ValueError indicating that no\n    valid years were provided.\n\n    Parameters\n    ----------\n    list_with_invalid_int_years : list\n        A list of integers representing years, all of which are invalid.\n    list_with_invalid_str_years : list\n        A list of strings, none of which are valid representations of\n        years.\n\n    Raises\n    ------\n    ValueError\n        If the list does not contain any valid years.\n    \"\"\"\n    with pytest.raises(ValueError) as excinfo:\n        collect_years_list(list_with_invalid_int_years)\n    assert (\n        f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n        in str(excinfo.value)\n    )\n\n    with pytest.raises(ValueError) as excinfo:\n        collect_years_list(list_with_invalid_str_years)\n    assert (\n        f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n        in str(excinfo.value)\n    )\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_all_valid_years","title":"<code>test_all_valid_years(list_with_valid_years)</code>","text":"<p>Test collect_years_list with a list containing only valid years.</p> <p>The function should return the same list of years if all are valid within the accepted range.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_all_valid_years--parameters","title":"Parameters","text":"<p>list_with_valid_years : list     A list of years that are all valid and within the collection range.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_all_valid_years(\n    self,\n    list_with_valid_years,\n):\n    \"\"\"Test collect_years_list with a list containing only valid years.\n\n    The function should return the same list of years if all are valid\n    within the accepted range.\n\n    Parameters\n    ----------\n    list_with_valid_years : list\n        A list of years that are all valid and within the collection range.\n    \"\"\"\n    result_list = collect_years_list(list_with_valid_years)\n\n    assert result_list == [2000, 2010, 2023]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_empty_list","title":"<code>test_empty_list()</code>","text":"<p>Test the behavior of collect_years_list when provided with an empty list.</p> <p>An empty list should trigger a ValueError to indicate that valid years are required for processing.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_empty_list--raises","title":"Raises","text":"<p>ValueError     If the provided list is empty.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_empty_list(self):\n    \"\"\"Test the behavior of collect_years_list when provided with an\n    empty list.\n\n    An empty list should trigger a ValueError to indicate that valid years\n    are required for processing.\n\n    Raises\n    ------\n    ValueError\n        If the provided list is empty.\n    \"\"\"\n    empty_list = []  # type: ignore\n    with pytest.raises(ValueError) as excinfo:\n        collect_years_list(empty_list)\n    assert (\n        f\"The list is empty. Provide a list with years after {first_year} and before {last_year}.\"  # noqa\n        in str(excinfo.value)\n    )\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_mixed_list","title":"<code>test_mixed_list(list_with_mix_int_years, list_with_mix_int_str_years, capsys)</code>","text":"<p>Test collect_years_list with lists that contain a mixture of valid and invalid years or strings.</p> <p>Valid years should be returned and invalid ones should be reported via standard output.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestCollectYearsList.test_mixed_list--parameters","title":"Parameters","text":"<p>list_with_mix_int_years : list     A list containing a mix of valid and invalid integer years. list_with_mix_int_str_years : list     A list containing both valid integer years and invalid string     years. capsys : fixture     Pytest fixture that captures standard output and error streams.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_mixed_list(\n    self,\n    list_with_mix_int_years,\n    list_with_mix_int_str_years,\n    capsys,\n):\n    \"\"\"Test collect_years_list with lists that contain a mixture of valid\n    and invalid years or strings.\n\n    Valid years should be returned and invalid ones should be reported via\n    standard output.\n\n    Parameters\n    ----------\n    list_with_mix_int_years : list\n        A list containing a mix of valid and invalid integer years.\n    list_with_mix_int_str_years : list\n        A list containing both valid integer years and invalid string\n        years.\n    capsys : fixture\n        Pytest fixture that captures standard output and error streams.\n    \"\"\"\n    result_with_int = collect_years_list(list_with_mix_int_years)\n\n    assert result_with_int == [2010, 2023]\n\n    assert (\n        capsys.readouterr().out\n        == \"The elements [1999] were removed from the list.\\n\"\n    )\n\n    result_with_str = collect_years_list(list_with_mix_int_str_years)\n\n    assert result_with_str == [2010, 2023]\n\n    assert (\n        capsys.readouterr().out\n        == \"The elements ['A'] were removed from the list.\\n\"\n    )\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector","title":"<code>TestStationDataCollector</code>","text":"Source code in <code>tests/tools/test_collectors.py</code> <pre><code>class TestStationDataCollector:\n    def test_collecting_empty_folder(\n        self,\n        tmp_path_factory,\n    ):\n        \"\"\"Ensure StationDataCollector raises an error when scanning an empty\n        folder.\n\n        Parameters\n        ----------\n        tmp_path_factory : fixture\n            Pytest fixture that provides a factory for temporary directories.\n\n        Asserts\n        ------\n        ValueError\n            Expected error when the collector encounters an empty folder.\n        \"\"\"\n        stage_empty = tmp_path_factory.mktemp(\"stage_empty\")\n        output_empty = tmp_path_factory.mktemp(\"output_empty\")\n\n        stations_data = StationDataCollector(\n            stage_empty,\n            output_empty,\n            STATIONS_FILE,\n            STATION_COLUMN_NAMES,\n            StationData,\n        )\n\n        output_file = STATIONS_FILE + \".parquet\"\n\n        output_empty_file_path = output_empty / output_file\n\n        with pytest.raises(ValueError) as excinfo:\n            stations_data.start()\n        assert \"No CSV files found in the specified folder.\" in str(\n            excinfo.value,\n        )\n        assert not os.path.exists(output_empty_file_path)\n\n    def test_collecting_valid_data(\n        self,\n        tmp_station_valid_data,\n    ):\n        \"\"\"Ensure StationDataCollector does not create a log file when\n        processing valid data.\n\n        Parameters\n        ----------\n        tmp_station_valid_data : fixture\n            Fixture providing a directory with valid CSV data.\n\n        Asserts\n        ------\n        bool\n            No log file is created and an output file exists.\n        \"\"\"\n        stage_valid, output_valid = tmp_station_valid_data\n\n        stations_data = StationDataCollector(\n            stage_valid,\n            output_valid,\n            STATIONS_FILE,\n            STATION_COLUMN_NAMES,\n            StationData,\n        )\n\n        stations_data.start()\n\n        log_file = STATIONS_FILE + \"_invalid_records.log\"\n\n        log_file_path = output_valid / log_file\n\n        output_file = STATIONS_FILE + \".parquet\"\n\n        output_valid_file_path = output_valid / output_file\n\n        assert not os.path.exists(log_file_path)\n        assert os.path.exists(output_valid_file_path)\n\n    def test_collecting_invalid_data(\n        self,\n        tmp_station_invalid_data,\n    ):\n        \"\"\"Ensure StationDataCollector creates a log file when processing\n        invalid data.\n\n        Parameters\n        ----------\n        tmp_station_invalid_data : fixture\n            Fixture providing a directory with invalid CSV data.\n\n        Asserts\n        ------\n        Exception\n            Expected error when the collector encounters only invalid data.\n        \"\"\"\n        stage_invalid, output_invalid = tmp_station_invalid_data\n\n        stations_data = StationDataCollector(\n            stage_invalid,\n            output_invalid,\n            STATIONS_FILE,\n            STATION_COLUMN_NAMES,\n            StationData,\n        )\n\n        output_file = STATIONS_FILE + \".parquet\"\n        output_invalid_file_path = output_invalid / output_file\n\n        with pytest.raises(Exception) as excinfo:\n            stations_data.start()\n        assert \"All collected data was invalid.\" in str(\n            excinfo.value,\n        )\n\n        log_files = glob.glob(os.path.join(output_invalid, \"*.log\"))\n\n        assert len(log_files) == 3\n        assert not os.path.exists(output_invalid_file_path)\n\n    def test_collecting_mixed_data(\n        self,\n        tmp_station_mixed_data,\n    ):\n        \"\"\"Ensure StationDataCollector handles mixed data correctly.\n\n        Parameters\n        ----------\n        tmp_station_mixed_data : fixture\n            Fixture providing a directory with mixed CSV data.\n\n        Asserts\n        ------\n        bool\n            A log file is created for invalid records and an output file\n            exists.\n        \"\"\"\n        stage_mixed, output_mixed = tmp_station_mixed_data\n\n        stations_data = StationDataCollector(\n            stage_mixed,\n            output_mixed,\n            STATIONS_FILE,\n            STATION_COLUMN_NAMES,\n            StationData,\n        )\n\n        output_file = STATIONS_FILE + \".parquet\"\n        output_mixed_file_path = output_mixed / output_file\n\n        stations_data.start()\n\n        log_files = glob.glob(os.path.join(output_mixed, \"*.log\"))\n\n        assert len(log_files) == 1\n\n        assert os.path.exists(output_mixed_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_empty_folder","title":"<code>test_collecting_empty_folder(tmp_path_factory)</code>","text":"<p>Ensure StationDataCollector raises an error when scanning an empty folder.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_empty_folder--parameters","title":"Parameters","text":"<p>tmp_path_factory : fixture     Pytest fixture that provides a factory for temporary directories.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_empty_folder--asserts","title":"Asserts","text":"<p>ValueError     Expected error when the collector encounters an empty folder.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_empty_folder(\n    self,\n    tmp_path_factory,\n):\n    \"\"\"Ensure StationDataCollector raises an error when scanning an empty\n    folder.\n\n    Parameters\n    ----------\n    tmp_path_factory : fixture\n        Pytest fixture that provides a factory for temporary directories.\n\n    Asserts\n    ------\n    ValueError\n        Expected error when the collector encounters an empty folder.\n    \"\"\"\n    stage_empty = tmp_path_factory.mktemp(\"stage_empty\")\n    output_empty = tmp_path_factory.mktemp(\"output_empty\")\n\n    stations_data = StationDataCollector(\n        stage_empty,\n        output_empty,\n        STATIONS_FILE,\n        STATION_COLUMN_NAMES,\n        StationData,\n    )\n\n    output_file = STATIONS_FILE + \".parquet\"\n\n    output_empty_file_path = output_empty / output_file\n\n    with pytest.raises(ValueError) as excinfo:\n        stations_data.start()\n    assert \"No CSV files found in the specified folder.\" in str(\n        excinfo.value,\n    )\n    assert not os.path.exists(output_empty_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_invalid_data","title":"<code>test_collecting_invalid_data(tmp_station_invalid_data)</code>","text":"<p>Ensure StationDataCollector creates a log file when processing invalid data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_invalid_data--parameters","title":"Parameters","text":"<p>tmp_station_invalid_data : fixture     Fixture providing a directory with invalid CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_invalid_data--asserts","title":"Asserts","text":"<p>Exception     Expected error when the collector encounters only invalid data.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_invalid_data(\n    self,\n    tmp_station_invalid_data,\n):\n    \"\"\"Ensure StationDataCollector creates a log file when processing\n    invalid data.\n\n    Parameters\n    ----------\n    tmp_station_invalid_data : fixture\n        Fixture providing a directory with invalid CSV data.\n\n    Asserts\n    ------\n    Exception\n        Expected error when the collector encounters only invalid data.\n    \"\"\"\n    stage_invalid, output_invalid = tmp_station_invalid_data\n\n    stations_data = StationDataCollector(\n        stage_invalid,\n        output_invalid,\n        STATIONS_FILE,\n        STATION_COLUMN_NAMES,\n        StationData,\n    )\n\n    output_file = STATIONS_FILE + \".parquet\"\n    output_invalid_file_path = output_invalid / output_file\n\n    with pytest.raises(Exception) as excinfo:\n        stations_data.start()\n    assert \"All collected data was invalid.\" in str(\n        excinfo.value,\n    )\n\n    log_files = glob.glob(os.path.join(output_invalid, \"*.log\"))\n\n    assert len(log_files) == 3\n    assert not os.path.exists(output_invalid_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_mixed_data","title":"<code>test_collecting_mixed_data(tmp_station_mixed_data)</code>","text":"<p>Ensure StationDataCollector handles mixed data correctly.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_mixed_data--parameters","title":"Parameters","text":"<p>tmp_station_mixed_data : fixture     Fixture providing a directory with mixed CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_mixed_data--asserts","title":"Asserts","text":"<p>bool     A log file is created for invalid records and an output file     exists.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_mixed_data(\n    self,\n    tmp_station_mixed_data,\n):\n    \"\"\"Ensure StationDataCollector handles mixed data correctly.\n\n    Parameters\n    ----------\n    tmp_station_mixed_data : fixture\n        Fixture providing a directory with mixed CSV data.\n\n    Asserts\n    ------\n    bool\n        A log file is created for invalid records and an output file\n        exists.\n    \"\"\"\n    stage_mixed, output_mixed = tmp_station_mixed_data\n\n    stations_data = StationDataCollector(\n        stage_mixed,\n        output_mixed,\n        STATIONS_FILE,\n        STATION_COLUMN_NAMES,\n        StationData,\n    )\n\n    output_file = STATIONS_FILE + \".parquet\"\n    output_mixed_file_path = output_mixed / output_file\n\n    stations_data.start()\n\n    log_files = glob.glob(os.path.join(output_mixed, \"*.log\"))\n\n    assert len(log_files) == 1\n\n    assert os.path.exists(output_mixed_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_valid_data","title":"<code>test_collecting_valid_data(tmp_station_valid_data)</code>","text":"<p>Ensure StationDataCollector does not create a log file when processing valid data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_valid_data--parameters","title":"Parameters","text":"<p>tmp_station_valid_data : fixture     Fixture providing a directory with valid CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestStationDataCollector.test_collecting_valid_data--asserts","title":"Asserts","text":"<p>bool     No log file is created and an output file exists.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_valid_data(\n    self,\n    tmp_station_valid_data,\n):\n    \"\"\"Ensure StationDataCollector does not create a log file when\n    processing valid data.\n\n    Parameters\n    ----------\n    tmp_station_valid_data : fixture\n        Fixture providing a directory with valid CSV data.\n\n    Asserts\n    ------\n    bool\n        No log file is created and an output file exists.\n    \"\"\"\n    stage_valid, output_valid = tmp_station_valid_data\n\n    stations_data = StationDataCollector(\n        stage_valid,\n        output_valid,\n        STATIONS_FILE,\n        STATION_COLUMN_NAMES,\n        StationData,\n    )\n\n    stations_data.start()\n\n    log_file = STATIONS_FILE + \"_invalid_records.log\"\n\n    log_file_path = output_valid / log_file\n\n    output_file = STATIONS_FILE + \".parquet\"\n\n    output_valid_file_path = output_valid / output_file\n\n    assert not os.path.exists(log_file_path)\n    assert os.path.exists(output_valid_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector","title":"<code>TestWeatherDataCollector</code>","text":"Source code in <code>tests/tools/test_collectors.py</code> <pre><code>class TestWeatherDataCollector:\n    def test_collecting_empty_folder(\n        self,\n        tmp_path_factory,\n    ):\n        \"\"\"Ensure WeatherDataCollector raises an error when scanning an empty\n        folder.\n\n        Parameters\n        ----------\n        tmp_path_factory : fixture\n            Pytest fixture that provides a factory for temporary directories.\n\n        Asserts\n        ------\n        ValueError\n            Expected error when the collector encounters an empty folder.\n        \"\"\"\n        stage_empty = tmp_path_factory.mktemp(\"stage_empty\")\n        output_empty = tmp_path_factory.mktemp(\"output_empty\")\n\n        weather_data = WeatherDataCollector(\n            stage_empty,\n            output_empty,\n            WEATHER_FILE,\n            WEATHER_COLUMN_NAMES,\n            WeatherData,\n        )\n\n        output_file = WEATHER_FILE + \".parquet\"\n\n        output_empty_file_path = output_empty / output_file\n\n        with pytest.raises(ValueError) as excinfo:\n            weather_data.start()\n        assert \"No CSV files found in the specified folder.\" in str(\n            excinfo.value,\n        )\n        assert not os.path.exists(output_empty_file_path)\n\n    def test_collecting_valid_data(\n        self,\n        tmp_station_valid_data,\n    ):\n        \"\"\"Ensure WeatherDataCollector does not create a log file when\n        processing valid data.\n\n        Parameters\n        ----------\n        tmp_station_valid_data : fixture\n            Fixture providing a directory with valid CSV data.\n\n        Asserts\n        ------\n        bool\n            No log file is created and an output file exists.\n        \"\"\"\n        stage_valid, output_valid = tmp_station_valid_data\n\n        weather_data = WeatherDataCollector(\n            stage_valid,\n            output_valid,\n            WEATHER_FILE,\n            WEATHER_COLUMN_NAMES,\n            WeatherData,\n        )\n\n        weather_data.start()\n\n        log_file = WEATHER_FILE + \"_invalid_records.log\"\n\n        log_file_path = output_valid / log_file\n\n        output_file = WEATHER_FILE + \".parquet\"\n\n        output_valid_file_path = output_valid / output_file\n\n        assert not os.path.exists(log_file_path)\n        assert os.path.exists(output_valid_file_path)\n\n    def test_collecting_invalid_data(\n        self,\n        tmp_station_invalid_data,\n    ):\n        \"\"\"Ensure WeatherDataCollector creates a log file when processing\n        invalid data.\n\n        Parameters\n        ----------\n        tmp_station_invalid_data : fixture\n            Fixture providing a directory with invalid CSV data.\n\n        Asserts\n        ------\n        Exception\n            Expected error when the collector encounters only invalid data.\n        \"\"\"\n        stage_invalid, output_invalid = tmp_station_invalid_data\n\n        weather_data = WeatherDataCollector(\n            stage_invalid,\n            output_invalid,\n            WEATHER_FILE,\n            WEATHER_COLUMN_NAMES,\n            WeatherData,\n        )\n\n        output_file = WEATHER_FILE + \".parquet\"\n        output_invalid_file_path = output_invalid / output_file\n\n        with pytest.raises(Exception) as excinfo:\n            weather_data.start()\n\n        log_files = glob.glob(os.path.join(output_invalid, \"*.log\"))\n\n        assert \"All collected data was invalid.\" in str(\n            excinfo.value,\n        )\n        assert len(log_files) == 3\n        assert not os.path.exists(output_invalid_file_path)\n\n    def test_collecting_mixed_data(\n        self,\n        tmp_station_mixed_data,\n    ):\n        \"\"\"Ensure WeatherDataCollector handles mixed data correctly.\n\n        Parameters\n        ----------\n        tmp_station_mixed_data : fixture\n            Fixture providing a directory with mixed CSV data.\n\n        Asserts\n        ------\n        bool\n            A log file is created for invalid records and an output file\n            exists.\n        \"\"\"\n        stage_mixed, output_mixed = tmp_station_mixed_data\n\n        weather_data = WeatherDataCollector(\n            stage_mixed,\n            output_mixed,\n            WEATHER_FILE,\n            WEATHER_COLUMN_NAMES,\n            WeatherData,\n        )\n\n        output_file = WEATHER_FILE + \".parquet\"\n        output_mixed_file_path = output_mixed / output_file\n\n        weather_data.start()\n\n        log_files = glob.glob(os.path.join(output_mixed, \"*.log\"))\n\n        assert len(log_files) == 2\n\n        assert os.path.exists(output_mixed_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_empty_folder","title":"<code>test_collecting_empty_folder(tmp_path_factory)</code>","text":"<p>Ensure WeatherDataCollector raises an error when scanning an empty folder.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_empty_folder--parameters","title":"Parameters","text":"<p>tmp_path_factory : fixture     Pytest fixture that provides a factory for temporary directories.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_empty_folder--asserts","title":"Asserts","text":"<p>ValueError     Expected error when the collector encounters an empty folder.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_empty_folder(\n    self,\n    tmp_path_factory,\n):\n    \"\"\"Ensure WeatherDataCollector raises an error when scanning an empty\n    folder.\n\n    Parameters\n    ----------\n    tmp_path_factory : fixture\n        Pytest fixture that provides a factory for temporary directories.\n\n    Asserts\n    ------\n    ValueError\n        Expected error when the collector encounters an empty folder.\n    \"\"\"\n    stage_empty = tmp_path_factory.mktemp(\"stage_empty\")\n    output_empty = tmp_path_factory.mktemp(\"output_empty\")\n\n    weather_data = WeatherDataCollector(\n        stage_empty,\n        output_empty,\n        WEATHER_FILE,\n        WEATHER_COLUMN_NAMES,\n        WeatherData,\n    )\n\n    output_file = WEATHER_FILE + \".parquet\"\n\n    output_empty_file_path = output_empty / output_file\n\n    with pytest.raises(ValueError) as excinfo:\n        weather_data.start()\n    assert \"No CSV files found in the specified folder.\" in str(\n        excinfo.value,\n    )\n    assert not os.path.exists(output_empty_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_invalid_data","title":"<code>test_collecting_invalid_data(tmp_station_invalid_data)</code>","text":"<p>Ensure WeatherDataCollector creates a log file when processing invalid data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_invalid_data--parameters","title":"Parameters","text":"<p>tmp_station_invalid_data : fixture     Fixture providing a directory with invalid CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_invalid_data--asserts","title":"Asserts","text":"<p>Exception     Expected error when the collector encounters only invalid data.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_invalid_data(\n    self,\n    tmp_station_invalid_data,\n):\n    \"\"\"Ensure WeatherDataCollector creates a log file when processing\n    invalid data.\n\n    Parameters\n    ----------\n    tmp_station_invalid_data : fixture\n        Fixture providing a directory with invalid CSV data.\n\n    Asserts\n    ------\n    Exception\n        Expected error when the collector encounters only invalid data.\n    \"\"\"\n    stage_invalid, output_invalid = tmp_station_invalid_data\n\n    weather_data = WeatherDataCollector(\n        stage_invalid,\n        output_invalid,\n        WEATHER_FILE,\n        WEATHER_COLUMN_NAMES,\n        WeatherData,\n    )\n\n    output_file = WEATHER_FILE + \".parquet\"\n    output_invalid_file_path = output_invalid / output_file\n\n    with pytest.raises(Exception) as excinfo:\n        weather_data.start()\n\n    log_files = glob.glob(os.path.join(output_invalid, \"*.log\"))\n\n    assert \"All collected data was invalid.\" in str(\n        excinfo.value,\n    )\n    assert len(log_files) == 3\n    assert not os.path.exists(output_invalid_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_mixed_data","title":"<code>test_collecting_mixed_data(tmp_station_mixed_data)</code>","text":"<p>Ensure WeatherDataCollector handles mixed data correctly.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_mixed_data--parameters","title":"Parameters","text":"<p>tmp_station_mixed_data : fixture     Fixture providing a directory with mixed CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_mixed_data--asserts","title":"Asserts","text":"<p>bool     A log file is created for invalid records and an output file     exists.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_mixed_data(\n    self,\n    tmp_station_mixed_data,\n):\n    \"\"\"Ensure WeatherDataCollector handles mixed data correctly.\n\n    Parameters\n    ----------\n    tmp_station_mixed_data : fixture\n        Fixture providing a directory with mixed CSV data.\n\n    Asserts\n    ------\n    bool\n        A log file is created for invalid records and an output file\n        exists.\n    \"\"\"\n    stage_mixed, output_mixed = tmp_station_mixed_data\n\n    weather_data = WeatherDataCollector(\n        stage_mixed,\n        output_mixed,\n        WEATHER_FILE,\n        WEATHER_COLUMN_NAMES,\n        WeatherData,\n    )\n\n    output_file = WEATHER_FILE + \".parquet\"\n    output_mixed_file_path = output_mixed / output_file\n\n    weather_data.start()\n\n    log_files = glob.glob(os.path.join(output_mixed, \"*.log\"))\n\n    assert len(log_files) == 2\n\n    assert os.path.exists(output_mixed_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_valid_data","title":"<code>test_collecting_valid_data(tmp_station_valid_data)</code>","text":"<p>Ensure WeatherDataCollector does not create a log file when processing valid data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_valid_data--parameters","title":"Parameters","text":"<p>tmp_station_valid_data : fixture     Fixture providing a directory with valid CSV data.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.TestWeatherDataCollector.test_collecting_valid_data--asserts","title":"Asserts","text":"<p>bool     No log file is created and an output file exists.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def test_collecting_valid_data(\n    self,\n    tmp_station_valid_data,\n):\n    \"\"\"Ensure WeatherDataCollector does not create a log file when\n    processing valid data.\n\n    Parameters\n    ----------\n    tmp_station_valid_data : fixture\n        Fixture providing a directory with valid CSV data.\n\n    Asserts\n    ------\n    bool\n        No log file is created and an output file exists.\n    \"\"\"\n    stage_valid, output_valid = tmp_station_valid_data\n\n    weather_data = WeatherDataCollector(\n        stage_valid,\n        output_valid,\n        WEATHER_FILE,\n        WEATHER_COLUMN_NAMES,\n        WeatherData,\n    )\n\n    weather_data.start()\n\n    log_file = WEATHER_FILE + \"_invalid_records.log\"\n\n    log_file_path = output_valid / log_file\n\n    output_file = WEATHER_FILE + \".parquet\"\n\n    output_valid_file_path = output_valid / output_file\n\n    assert not os.path.exists(log_file_path)\n    assert os.path.exists(output_valid_file_path)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.create_files","title":"<code>create_files(destiny_folder, data_to_files)</code>","text":"<p>Create CSV files from a list of CSV content strings in a specified folder.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.create_files--parameters","title":"Parameters","text":"<p>destiny_folder : pathlib.Path     Destination folder where CSV files will be created. data_to_files : List[str]     List of strings, each representing the content of a CSV file.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>def create_files(\n    destiny_folder: pathlib.Path,\n    data_to_files: List[str],\n) -&gt; None:\n    \"\"\"Create CSV files from a list of CSV content strings in a specified\n    folder.\n\n    Parameters\n    ----------\n    destiny_folder : pathlib.Path\n        Destination folder where CSV files will be created.\n    data_to_files : List[str]\n        List of strings, each representing the content of a CSV file.\n    \"\"\"\n    for file_number, csv_data in enumerate(data_to_files):\n        file_name = \"file_\" + str(file_number) + \".csv\"\n        file = destiny_folder / file_name\n        file.write_text(csv_data)\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_invalid_int_years","title":"<code>list_with_invalid_int_years()</code>","text":"<p>Provide a list of integer years that are not valid for data collection, as they precede the established start year.</p> <p>This fixture is used to simulate scenarios where the data collector receives years that are outside the scope of the collection period.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_invalid_int_years--returns","title":"Returns","text":"<p>list     A list containing integer years that fall before the first year from     which data collection is considered valid.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef list_with_invalid_int_years():\n    \"\"\"Provide a list of integer years that are not valid for data collection,\n    as they precede the established start year.\n\n    This fixture is used to simulate scenarios where the data collector\n    receives years that are outside the scope of the collection period.\n\n    Returns\n    -------\n    list\n        A list containing integer years that fall before the first year from\n        which data collection is considered valid.\n    \"\"\"\n    return [1998, 1988, 1999]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_invalid_str_years","title":"<code>list_with_invalid_str_years()</code>","text":"<p>Supply a list of strings that are not valid year representations for testing the data collector's year filtering.</p> <p>This fixture is useful for testing the data collector's ability to ignore non-integer inputs when filtering years.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_invalid_str_years--returns","title":"Returns","text":"<p>list     A list containing strings that are invalid as year inputs.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef list_with_invalid_str_years():\n    \"\"\"Supply a list of strings that are not valid year representations for\n    testing the data collector's year filtering.\n\n    This fixture is useful for testing the data collector's ability to ignore\n    non-integer inputs when filtering years.\n\n    Returns\n    -------\n    list\n        A list containing strings that are invalid as year inputs.\n    \"\"\"\n    return [\"A\", \"B\", \"C\"]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_mix_int_str_years","title":"<code>list_with_mix_int_str_years()</code>","text":"<p>Produce a list with a combination of valid integer years and invalid string representations for testing year filtering.</p> <p>This fixture aids in testing the data collector's robustness in handling mixed data types in year lists.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_mix_int_str_years--returns","title":"Returns","text":"<p>list     A list that includes both valid integer years and strings that     represent invalid years.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef list_with_mix_int_str_years():\n    \"\"\"Produce a list with a combination of valid integer years and invalid\n    string representations for testing year filtering.\n\n    This fixture aids in testing the data collector's robustness in handling\n    mixed data types in year lists.\n\n    Returns\n    -------\n    list\n        A list that includes both valid integer years and strings that\n        represent invalid years.\n    \"\"\"\n    return [\"A\", 2010, 2023]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_mix_int_years","title":"<code>list_with_mix_int_years()</code>","text":"<p>Generate a list with a mix of valid and invalid integer years for testing the data collector's filtering logic.</p> <p>This fixture helps to ensure that the data collector correctly identifies and filters out years that are not within the valid range.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_mix_int_years--returns","title":"Returns","text":"<p>list     A list containing a combination of valid and invalid integer years.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef list_with_mix_int_years():\n    \"\"\"Generate a list with a mix of valid and invalid integer years for\n    testing the data collector's filtering logic.\n\n    This fixture helps to ensure that the data collector correctly identifies\n    and filters out years that are not within the valid range.\n\n    Returns\n    -------\n    list\n        A list containing a combination of valid and invalid integer years.\n    \"\"\"\n    return [1999, 2010, 2023]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_valid_years","title":"<code>list_with_valid_years()</code>","text":"<p>Provide a list of integer years that are all within the valid data collection range for testing.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.list_with_valid_years--returns","title":"Returns","text":"<p>list     A list containing only valid integer years, starting from the first     year data collection is considered valid.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef list_with_valid_years():\n    \"\"\"Provide a list of integer years that are all within the valid data\n    collection range for testing.\n\n    Returns\n    -------\n    list\n        A list containing only valid integer years, starting from the first\n        year data collection is considered valid.\n    \"\"\"\n    return [2000, 2010, 2023]\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_invalid_data","title":"<code>tmp_station_invalid_data(tmp_path)</code>","text":"<p>Create a temporary directory with invalid station data for testing.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_invalid_data--parameters","title":"Parameters","text":"<p>tmp_path : py.path.local     Fixture provided by pytest to create and return temporary directories.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_invalid_data--yields","title":"Yields","text":"<p>tuple     A tuple containing the path to the invalid staging data and the output directory.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef tmp_station_invalid_data(tmp_path):\n    \"\"\"Create a temporary directory with invalid station data for testing.\n\n    Parameters\n    ----------\n    tmp_path : py.path.local\n        Fixture provided by pytest to create and return temporary directories.\n\n    Yields\n    ------\n    tuple\n        A tuple containing the path to the invalid staging data and the output directory.\n    \"\"\"\n    temp_stage_invalid = tmp_path / \"stage-invalid\"\n    temp_stage_invalid.mkdir()\n    temp_output_invalid = tmp_path / \"output-invalid\"\n    temp_output_invalid.mkdir()\n\n    csv_data_infos = [\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRASILIA\\n\"\n        \"CODIGO (WMO):;A001\\n\"\n        \"LATITUDE:;aaaaaaaaaa\\n\"\n        \"LONGITUDE:;-47,92583332\\n\"\n        \"ALTITUDE:;1160,96\\n\"\n        \"DATA DE FUNDACAO:;07/05/00\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"\n        \"2023/01/01;0000 ;0;887,7;887,7;887,2;;20,1;17,9;20,9;20;19,2;17,8;91;87;87;187;3,3;1,2;\\n\"\n        \"2023/01/01;0100 ;0;888,1;888,1;887,7;;19,2;17,5;20,1;19,2;17,8;17,4;90;87;90;153;2,9;,8;\\n\"\n        \"2023/01/01;0200 ;0;887,8;888,1;887,8;;19,3;17,6;19,5;19;17,8;17,3;90;89;90;145;2,5;1,5;\\n\"\n        \"2023/01/01;0300 ;0;887,8;887,9;887,7;;19,3;17,7;19,4;19,1;17,8;17,5;91;90;91;162;3,2;1,4;\\n\"\n        \"2023/01/01;0400 ;0;887,6;887,9;887,6;;19,7;18,1;19,7;19,1;18,1;17,4;91;90;90;140;5,7;2,7;\\n\"\n        \"2023/01/01;0500 ;0;886,7;887,6;886,7;;19,1;17,7;19,7;19,1;18,1;17,7;92;90;92;128;7,1;2;\"\n        \"\",\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRAZLANDIA\\n\"\n        \"CODIGO (WMO):;A042\\n\"\n        \"LATITUDE:;-15,59972221\\n\"\n        \"LONGITUDE:;aaaaaaa\\n\"\n        \"ALTITUDE:;1143\\n\"\n        \"DATA DE FUNDACAO:;19/07/17\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"  # noqa\n        \"/01/01;0000 UTC;0;888,7;888,9;888,2;;20,5;18;21;20,5;18,4;18;87;84;86;146;3,9;1,8;\\n\"\n        \"/01/01;0100 UTC;0;889;889;888,7;;20,4;17,5;20,5;20,3;18;17,5;86;84;84;142;4,1;1,8;\\n\"\n        \"/01/01;0200 UTC;0;888,9;889,1;888,9;;19,9;17,4;20,4;19,8;17,5;17,3;86;83;86;153;4;,2;\\n\"\n        \"/01/01;0300 UTC;0;888,7;888,9;888,7;;19,8;17,4;19,9;19,7;17,4;17,2;87;85;86;140;3,5;1,2;\\n\"\n        \"/01/01;0400 UTC;0;888,4;888,8;888,4;;20,1;17,3;20,1;19,8;17,5;17,3;86;84;84;139;3,3;,3;\\n\"\n        \"/01/01;0500 UTC;0;887,8;888,4;887,8;;19,8;17,6;20,3;19,8;17,6;17,2;87;83;87;138;5,2;1,3;\\n\"\n        \"/01/01;0600 UTC;0;887,5;887,9;887,5;;19,6;17,3;19,8;19,5;17,7;17,3;89;87;87;133;4,1;,3;\\n\"\n        \"/01/01;0700 UTC;0;887,5;887,5;887,3;;19,1;17,3;19,6;19,1;17,3;17,2;89;87;89;95;4,4;1,4;\\n\"\n        \"/01/01;0800 UTC;0;887,6;887,6;887,5;;18,9;17,1;19,2;18,3;17,5;16,9;92;89;89;49;5,5;2,4;\\n\"\n        \"/01/01;0900 UTC;0;888;888;887,4;5,3;18,8;17;19,1;18,4;17,1;16,6;90;88;89;73;4,8;,6;\",\n        \"REGIAO:;N\\n\"\n        \"UF:;AM\\n\"\n        \"ESTACAO:;COARI\\n\"\n        \"CODIGO (WMO):;A117\\n\"\n        \"LATITUDE:;-4,09749999\\n\"\n        \"LONGITUDE:;aaaaaaaa\\n\"\n        \"ALTITUDE:;33,84\\n\"\n        \"DATA DE FUNDACAO:;12/04/08\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"\n        \"2023//01;0000 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023//01;0100 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023//01;0200 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023//01;0300 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023//01;0400 UTC;;;;;;;;;;;;;;;;;;\",\n    ]\n\n    create_files(temp_stage_invalid, csv_data_infos)\n\n    yield temp_stage_invalid, temp_output_invalid\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_mixed_data","title":"<code>tmp_station_mixed_data(tmp_path)</code>","text":"<p>Create a temporary directory with mixed (valid and invalid) station data for testing.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_mixed_data--parameters","title":"Parameters","text":"<p>tmp_path : py.path.local     Fixture provided by pytest to create and return temporary directories.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_mixed_data--yields","title":"Yields","text":"<p>tuple     A tuple containing the path to the mixed staging data and the output directory.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef tmp_station_mixed_data(tmp_path):\n    \"\"\"Create a temporary directory with mixed (valid and invalid) station data for testing.\n\n    Parameters\n    ----------\n    tmp_path : py.path.local\n        Fixture provided by pytest to create and return temporary directories.\n\n    Yields\n    ------\n    tuple\n        A tuple containing the path to the mixed staging data and the output directory.\n    \"\"\"\n    temp_stage_mixed = tmp_path / \"stage-mixed\"\n    temp_stage_mixed.mkdir()\n    temp_output_mixed = tmp_path / \"output-mixed\"\n    temp_output_mixed.mkdir()\n\n    csv_data_infos = [\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRASILIA\\n\"\n        \"CODIGO (WMO):;A001\\n\"\n        \"LATITUDE:;-15,78944444\\n\"\n        \"LONGITUDE:;-47,92583332\\n\"\n        \"ALTITUDE:;1160,96\\n\"\n        \"DATA DE FUNDACAO:;07/05/00\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"  # noqa\n        \"2023/01/01;0000 UTC;0;887,7;887,7;887,2;;20,1;17,9;20,9;20;19,2;17,8;91;87;87;187;3,3;1,2;\\n\"\n        \"2023/01/01;0100 UTC;0;888,1;888,1;887,7;;19,2;17,5;20,1;19,2;17,8;17,4;90;87;90;153;2,9;,8;\\n\"\n        \"2023/01/01;0200 UTC;0;887,8;888,1;887,8;;19,3;17,6;19,5;19;17,8;17,3;90;89;90;145;2,5;1,5;\\n\"\n        \"2023/01/01;0300 UTC;0;887,8;887,9;887,7;;19,3;17,7;19,4;19,1;17,8;17,5;91;90;91;162;3,2;1,4;\\n\"\n        \"2023/01/01;0400 UTC;0;887,6;887,9;887,6;;19,7;18,1;19,7;19,1;18,1;17,4;91;90;90;140;5,7;2,7;\\n\"\n        \"2023/01/01;0500 UTC;0;886,7;887,6;886,7;;19,1;17,7;19,7;19,1;18,1;17,7;92;90;92;128;7,1;2;\",\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRAZLANDIA\\n\"\n        \"CODIGO (WMO):;AAAA\\n\"\n        \"LATITUDE:;-15,59972221\\n\"\n        \"LONGITUDE:;-48,1311111\\n\"\n        \"ALTITUDE:;1143\\n\"\n        \"DATA DE FUNDACAO:;19/07/17\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"  # noqa\n        \"2023/01/01;0000 UTC;0;888,7;888,9;888,2;;20,5;18;21;20,5;18,4;18;87;84;86;146;3,9;1,8;\\n\"\n        \"2023/01/01;0100 UTC;0;889;889;888,7;;20,4;17,5;20,5;20,3;18;17,5;86;84;84;142;4,1;1,8;\\n\"\n        \"2023/01/01;0200 UTC;0;888,9;889,1;888,9;;19,9;17,4;20,4;19,8;17,5;17,3;86;83;86;153;4;,2;\\n\"\n        \"2023/01/01;0300 UTC;0;888,7;888,9;888,7;;19,8;17,4;19,9;19,7;17,4;17,2;87;85;86;140;3,5;1,2;\\n\"\n        \"2023/01/01;0400 UTC;0;888,4;888,8;888,4;;20,1;17,3;20,1;19,8;17,5;17,3;86;84;84;139;3,3;,3;\\n\"\n        \"2023/01/01;0500 UTC;0;887,8;888,4;887,8;;19,8;17,6;20,3;19,8;17,6;17,2;87;83;87;138;5,2;1,3;\\n\"\n        \"2023/01/01;0600 UTC;0;887,5;887,9;887,5;;19,6;17,3;19,8;19,5;17,7;17,3;89;87;87;133;4,1;,3;\\n\"\n        \"2023/01/01;0700 UTC;0;887,5;887,5;887,3;;19,1;17,3;19,6;19,1;17,3;17,2;89;87;89;95;4,4;1,4;\\n\"\n        \"2023/01/01;0800 UTC;0;887,6;887,6;887,5;;18,9;17,1;19,2;18,3;17,5;16,9;92;89;89;49;5,5;2,4;\\n\"\n        \"2023/01/01;0900 UTC;0;888;888;887,4;5,3;18,8;17;19,1;18,4;17,1;16,6;90;88;89;73;4,8;,6;\",\n        \"REGIAO:;N\\n\"\n        \"UF:;AM\\n\"\n        \"ESTACAO:;COARI\\n\"\n        \"CODIGO (WMO):;A117\\n\"\n        \"LATITUDE:;-4,09749999\\n\"\n        \"LONGITUDE:;-63,14527777\\n\"\n        \"ALTITUDE:;33,84\\n\"\n        \"DATA DE FUNDACAO:;12/04/08\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"\n        \"/01/01;0000 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"/01/01;0100 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"/01/01;0200 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"/01/01;0300 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"/01/01;0400 UTC;;;;;;;;;;;;;;;;;;\",\n    ]\n\n    create_files(temp_stage_mixed, csv_data_infos)\n\n    yield temp_stage_mixed, temp_output_mixed\n</code></pre>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_valid_data","title":"<code>tmp_station_valid_data(tmp_path)</code>","text":"<p>Create a temporary directory with valid station data for testing.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_valid_data--parameters","title":"Parameters","text":"<p>tmp_path : py.path.local     Fixture provided by pytest to create and return temporary directories.</p>"},{"location":"tests/tools/test_collectors/#tests.tools.test_collectors.tmp_station_valid_data--yields","title":"Yields","text":"<p>tuple     A tuple containing the path to the valid staging data and the output     directory.</p> Source code in <code>tests/tools/test_collectors.py</code> <pre><code>@pytest.fixture()\ndef tmp_station_valid_data(tmp_path):\n    \"\"\"Create a temporary directory with valid station data for testing.\n\n    Parameters\n    ----------\n    tmp_path : py.path.local\n        Fixture provided by pytest to create and return temporary directories.\n\n    Yields\n    ------\n    tuple\n        A tuple containing the path to the valid staging data and the output\n        directory.\n    \"\"\"\n    temp_stage_valid = tmp_path / \"stage-valid\"\n    temp_stage_valid.mkdir()\n    temp_output_valid = tmp_path / \"output-valid\"\n    temp_output_valid.mkdir()\n    csv_data_infos = [\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRASILIA\\n\"\n        \"CODIGO (WMO):;A001\\n\"\n        \"LATITUDE:;-15,78944444\\n\"\n        \"LONGITUDE:;-47,92583332\\n\"\n        \"ALTITUDE:;1160,96\\n\"\n        \"DATA DE FUNDACAO:;07/05/00\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"  # noqa\n        \"2023/01/01;0000 UTC;0;887,7;887,7;887,2;;20,1;17,9;20,9;20;19,2;17,8;91;87;87;187;3,3;1,2;\\n\"\n        \"2023/01/01;0100 UTC;0;888,1;888,1;887,7;;19,2;17,5;20,1;19,2;17,8;17,4;90;87;90;153;2,9;,8;\\n\"\n        \"2023/01/01;0200 UTC;0;887,8;888,1;887,8;;19,3;17,6;19,5;19;17,8;17,3;90;89;90;145;2,5;1,5;\\n\"\n        \"2023/01/01;0300 UTC;0;887,8;887,9;887,7;;19,3;17,7;19,4;19,1;17,8;17,5;91;90;91;162;3,2;1,4;\\n\"\n        \"2023/01/01;0400 UTC;0;887,6;887,9;887,6;;19,7;18,1;19,7;19,1;18,1;17,4;91;90;90;140;5,7;2,7;\\n\"\n        \"2023/01/01;0500 UTC;0;886,7;887,6;886,7;;19,1;17,7;19,7;19,1;18,1;17,7;92;90;92;128;7,1;2;\",\n        \"REGIAO:;CO\\n\"\n        \"UF:;DF\\n\"\n        \"ESTACAO:;BRAZLANDIA\\n\"\n        \"CODIGO (WMO):;A042\\n\"\n        \"LATITUDE:;-15,59972221\\n\"\n        \"LONGITUDE:;-48,1311111\\n\"\n        \"ALTITUDE:;1143\\n\"\n        \"DATA DE FUNDACAO:;19/07/17\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"  # noqa\n        \"2023/01/01;0000 UTC;0;888,7;888,9;888,2;;20,5;18;21;20,5;18,4;18;87;84;86;146;3,9;1,8;\\n\"\n        \"2023/01/01;0100 UTC;0;889;889;888,7;;20,4;17,5;20,5;20,3;18;17,5;86;84;84;142;4,1;1,8;\\n\"\n        \"2023/01/01;0200 UTC;0;888,9;889,1;888,9;;19,9;17,4;20,4;19,8;17,5;17,3;86;83;86;153;4;,2;\\n\"\n        \"2023/01/01;0300 UTC;0;888,7;888,9;888,7;;19,8;17,4;19,9;19,7;17,4;17,2;87;85;86;140;3,5;1,2;\\n\"\n        \"2023/01/01;0400 UTC;0;888,4;888,8;888,4;;20,1;17,3;20,1;19,8;17,5;17,3;86;84;84;139;3,3;,3;\\n\"\n        \"2023/01/01;0500 UTC;0;887,8;888,4;887,8;;19,8;17,6;20,3;19,8;17,6;17,2;87;83;87;138;5,2;1,3;\\n\"\n        \"2023/01/01;0600 UTC;0;887,5;887,9;887,5;;19,6;17,3;19,8;19,5;17,7;17,3;89;87;87;133;4,1;,3;\\n\"\n        \"2023/01/01;0700 UTC;0;887,5;887,5;887,3;;19,1;17,3;19,6;19,1;17,3;17,2;89;87;89;95;4,4;1,4;\\n\"\n        \"2023/01/01;0800 UTC;0;887,6;887,6;887,5;;18,9;17,1;19,2;18,3;17,5;16,9;92;89;89;49;5,5;2,4;\\n\"\n        \"2023/01/01;0900 UTC;0;888;888;887,4;5,3;18,8;17;19,1;18,4;17,1;16,6;90;88;89;73;4,8;,6;\",\n        \"REGIAO:;N\\n\"\n        \"UF:;AM\\n\"\n        \"ESTACAO:;COARI\\n\"\n        \"CODIGO (WMO):;A117\\n\"\n        \"LATITUDE:;-4,09749999\\n\"\n        \"LONGITUDE:;-63,14527777\\n\"\n        \"ALTITUDE:;33,84\\n\"\n        \"DATA DE FUNDACAO:;12/04/08\\n\"\n        \"Data;Hora UTC;PRECIPITA\u00c7\u00c3O TOTAL, HOR\u00c1RIO (mm);PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB);PRESS\u00c3O ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB);PRESS\u00c3O ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB);RADIACAO GLOBAL (Kj/m\u00b2);TEMPERATURA DO AR - BULBO SECO, HORARIA (\u00b0C);TEMPERATURA DO PONTO DE ORVALHO (\u00b0C);TEMPERATURA M\u00c1XIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA M\u00cdNIMA NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (\u00b0C);TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (\u00b0C);UMIDADE REL. MAX. NA HORA ANT. (AUT) (%);UMIDADE REL. MIN. NA HORA ANT. (AUT) (%);UMIDADE RELATIVA DO AR, HORARIA (%);VENTO, DIRE\u00c7\u00c3O HORARIA (gr) (\u00b0 (gr));VENTO, RAJADA MAXIMA (m/s);VENTO, VELOCIDADE HORARIA (m/s);\\n\"\n        \"2023/01/01;0000 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023/01/01;0100 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023/01/01;0200 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023/01/01;0300 UTC;;;;;;;;;;;;;;;;;;\\n\"\n        \"2023/01/01;0400 UTC;;;;;;;;;;;;;;;;;;\",\n    ]\n\n    create_files(temp_stage_valid, csv_data_infos)\n\n    yield temp_stage_valid, temp_output_valid\n</code></pre>"},{"location":"tests/tools/test_general/","title":"Test Tools &gt; General","text":""},{"location":"tests/tools/test_general/#tests.tools.test_general.test_clear_folder","title":"<code>test_clear_folder(tools_temp_folder)</code>","text":"<p>Confirm that the clear_folder function removes all files and subdirectories from a folder.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_clear_folder--parameters","title":"Parameters","text":"<p>tools_temp_folder : str     Path to a temporary folder provided by the fixture.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_clear_folder--asserts","title":"Asserts","text":"<p>The folder is empty after the clear_folder function is executed.</p> Source code in <code>tests/tools/test_general.py</code> <pre><code>def test_clear_folder(tools_temp_folder):\n    \"\"\"Confirm that the clear_folder function removes all files and\n    subdirectories from a folder.\n\n    Parameters\n    ----------\n    tools_temp_folder : str\n        Path to a temporary folder provided by the fixture.\n\n    Asserts\n    -------\n    The folder is empty after the clear_folder function is executed.\n    \"\"\"\n    test_zip_path = tools_temp_folder\n\n    extract_to = os.path.join(test_zip_path, \"extracted_data\")\n\n    clear_folder(extract_to)\n\n    files_extracted = len(\n        [\n            name\n            for name in os.listdir(extract_to)\n            if os.path.isfile(os.path.join(extract_to, name))\n        ],\n    )\n\n    assert files_extracted == 0\n</code></pre>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_download_file","title":"<code>test_download_file(tools_temp_folder)</code>","text":"<p>Verify that the download_file function downloads and saves a file correctly.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_download_file--parameters","title":"Parameters","text":"<p>tools_temp_folder : str     Path to a temporary folder provided by the fixture.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_download_file--asserts","title":"Asserts","text":"<p>The file is saved to the specified path with the correct content.</p> Source code in <code>tests/tools/test_general.py</code> <pre><code>def test_download_file(tools_temp_folder):\n    \"\"\"Verify that the download_file function downloads and saves a\n    file correctly.\n\n    Parameters\n    ----------\n    tools_temp_folder : str\n        Path to a temporary folder provided by the fixture.\n\n    Asserts\n    -------\n    The file is saved to the specified path with the correct content.\n    \"\"\"\n\n    test_url = \"http://example.com/data/\"\n    test_file_name = \"test_file.zip\"\n    test_save_path = tools_temp_folder\n    fake_content = b\"Test content\"\n\n    mock_response = MagicMock()\n    mock_response.iter_bytes = lambda chunk_size: [fake_content]\n\n    test_file_path = os.path.join(test_save_path, test_file_name)\n\n    with patch(\"httpx.get\", return_value=mock_response):\n        download_file(test_url, test_file_name, test_save_path)\n\n        assert os.path.exists(test_file_path)\n        with open(test_file_path, \"rb\") as file:\n            content = file.read()\n            assert content == fake_content\n</code></pre>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_extract_zip","title":"<code>test_extract_zip(tools_temp_folder)</code>","text":"<p>Verify that the extract_zip function correctly extracts files from a zip archive.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_extract_zip--parameters","title":"Parameters","text":"<p>tools_temp_folder : str     Path to a temporary folder provided by the fixture.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.test_extract_zip--asserts","title":"Asserts","text":"<p>The zip archive is extracted to the target directory with all files.</p> Source code in <code>tests/tools/test_general.py</code> <pre><code>def test_extract_zip(tools_temp_folder):\n    \"\"\"Verify that the extract_zip function correctly extracts files\n    from a zip archive.\n\n    Parameters\n    ----------\n    tools_temp_folder : str\n        Path to a temporary folder provided by the fixture.\n\n    Asserts\n    -------\n    The zip archive is extracted to the target directory with all files.\n    \"\"\"\n    test_zip_path = tools_temp_folder\n    test_zip_file = \"file.zip\"\n\n    extract_to = os.path.join(test_zip_path, \"extracted_data\")\n\n    extract_zip(test_zip_path, test_zip_file, extract_to)\n\n    expected_file_path = os.path.join(extract_to, \"test.txt\")\n\n    files_extracted = len(\n        [\n            name\n            for name in os.listdir(extract_to)\n            if os.path.isfile(os.path.join(extract_to, name))\n        ],\n    )\n\n    assert os.path.exists(expected_file_path)\n\n    assert files_extracted == 1\n</code></pre>"},{"location":"tests/tools/test_general/#tests.tools.test_general.tools_temp_folder","title":"<code>tools_temp_folder()</code>","text":"<p>Provide a temporary folder for running tests.</p>"},{"location":"tests/tools/test_general/#tests.tools.test_general.tools_temp_folder--yields","title":"Yields","text":"<p>str     The path to the temporary folder created for the duration of the     module's tests.</p> Source code in <code>tests/tools/test_general.py</code> <pre><code>@pytest.fixture(scope=\"module\")\ndef tools_temp_folder():\n    \"\"\"Provide a temporary folder for running tests.\n\n    Yields\n    ------\n    str\n        The path to the temporary folder created for the duration of the\n        module's tests.\n    \"\"\"\n    with TemporaryDirectory() as test_tools:\n        test_zip_file = \"file.zip\"\n        zip_file_path = os.path.join(test_tools, test_zip_file)\n        with zipfile.ZipFile(zip_file_path, \"w\") as zip_file:\n            zip_file.writestr(\"test.txt\", \"This is a test file\")\n\n        yield test_tools\n</code></pre>"},{"location":"tests/tools/test_validators/","title":"Test Tools &gt; Validators","text":""},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists","title":"<code>TestValidateSublists</code>","text":"Source code in <code>tests/tools/test_validators.py</code> <pre><code>class TestValidateSublists:\n    def test_equal_elements_and_order(\n        self,\n        list_with_equal_elements_equal_order,\n    ):\n        \"\"\"Verify validate_sublists function with lists having identical\n        elements in the same order.\n\n        Parameters\n        ----------\n        list_with_equal_elements_equal_order : list\n            Input provided by a fixture.\n\n        Asserts\n        -------\n        The function returns True for lists with identical elements and order.\n        \"\"\"\n        assert validate_sublists(list_with_equal_elements_equal_order)\n\n    def test_equal_elements_and_different_order(\n        self,\n        list_with_equal_elements_different_order,\n    ):\n        \"\"\"Verify validate_sublists function with lists having identical\n        elements in different orders.\n\n        Parameters\n        ----------\n        list_with_equal_elements_different_order : list\n            Input provided by a fixture.\n\n        Asserts\n        -------\n        The function returns True for lists with identical elements\n        regardless of their order.\n        \"\"\"\n        assert validate_sublists(list_with_equal_elements_different_order)\n\n    def test_unequal_elements(self, list_with_unequal_elements):\n        \"\"\"Verify validate_sublists function with lists having different\n        elements.\n\n        Parameters\n        ----------\n        list_with_unequal_elements : list\n            Input provided by a fixture.\n\n        Asserts\n        -------\n        The function raises a ValueError for lists with differing elements.\n        \"\"\"\n        with pytest.raises(ValueError) as excinfo:\n            validate_sublists(list_with_unequal_elements)\n        assert \"Sublists do not have the same elements.\" in str(excinfo.value)\n\n    def test_unequal_elements_and_unbalance(\n        self,\n        list_with_unequal_elements_and_unbalance,\n    ):\n        \"\"\"Verify validate_sublists function with lists having different\n        elements and lengths.\n\n        Parameters\n        ----------\n        list_with_unequal_elements_and_unbalance : list\n            Input provided by a fixture.\n\n        Asserts\n        -------\n        The function raises a ValueError for lists with differing elements\n        and lengths.\n        \"\"\"\n        with pytest.raises(ValueError) as excinfo:\n            validate_sublists(list_with_unequal_elements_and_unbalance)\n        assert \"Sublists do not have the same elements.\" in str(excinfo.value)\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_different_order","title":"<code>test_equal_elements_and_different_order(list_with_equal_elements_different_order)</code>","text":"<p>Verify validate_sublists function with lists having identical elements in different orders.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_different_order--parameters","title":"Parameters","text":"<p>list_with_equal_elements_different_order : list     Input provided by a fixture.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_different_order--asserts","title":"Asserts","text":"<p>The function returns True for lists with identical elements regardless of their order.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>def test_equal_elements_and_different_order(\n    self,\n    list_with_equal_elements_different_order,\n):\n    \"\"\"Verify validate_sublists function with lists having identical\n    elements in different orders.\n\n    Parameters\n    ----------\n    list_with_equal_elements_different_order : list\n        Input provided by a fixture.\n\n    Asserts\n    -------\n    The function returns True for lists with identical elements\n    regardless of their order.\n    \"\"\"\n    assert validate_sublists(list_with_equal_elements_different_order)\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_order","title":"<code>test_equal_elements_and_order(list_with_equal_elements_equal_order)</code>","text":"<p>Verify validate_sublists function with lists having identical elements in the same order.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_order--parameters","title":"Parameters","text":"<p>list_with_equal_elements_equal_order : list     Input provided by a fixture.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_equal_elements_and_order--asserts","title":"Asserts","text":"<p>The function returns True for lists with identical elements and order.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>def test_equal_elements_and_order(\n    self,\n    list_with_equal_elements_equal_order,\n):\n    \"\"\"Verify validate_sublists function with lists having identical\n    elements in the same order.\n\n    Parameters\n    ----------\n    list_with_equal_elements_equal_order : list\n        Input provided by a fixture.\n\n    Asserts\n    -------\n    The function returns True for lists with identical elements and order.\n    \"\"\"\n    assert validate_sublists(list_with_equal_elements_equal_order)\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements","title":"<code>test_unequal_elements(list_with_unequal_elements)</code>","text":"<p>Verify validate_sublists function with lists having different elements.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements--parameters","title":"Parameters","text":"<p>list_with_unequal_elements : list     Input provided by a fixture.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements--asserts","title":"Asserts","text":"<p>The function raises a ValueError for lists with differing elements.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>def test_unequal_elements(self, list_with_unequal_elements):\n    \"\"\"Verify validate_sublists function with lists having different\n    elements.\n\n    Parameters\n    ----------\n    list_with_unequal_elements : list\n        Input provided by a fixture.\n\n    Asserts\n    -------\n    The function raises a ValueError for lists with differing elements.\n    \"\"\"\n    with pytest.raises(ValueError) as excinfo:\n        validate_sublists(list_with_unequal_elements)\n    assert \"Sublists do not have the same elements.\" in str(excinfo.value)\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements_and_unbalance","title":"<code>test_unequal_elements_and_unbalance(list_with_unequal_elements_and_unbalance)</code>","text":"<p>Verify validate_sublists function with lists having different elements and lengths.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements_and_unbalance--parameters","title":"Parameters","text":"<p>list_with_unequal_elements_and_unbalance : list     Input provided by a fixture.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.TestValidateSublists.test_unequal_elements_and_unbalance--asserts","title":"Asserts","text":"<p>The function raises a ValueError for lists with differing elements and lengths.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>def test_unequal_elements_and_unbalance(\n    self,\n    list_with_unequal_elements_and_unbalance,\n):\n    \"\"\"Verify validate_sublists function with lists having different\n    elements and lengths.\n\n    Parameters\n    ----------\n    list_with_unequal_elements_and_unbalance : list\n        Input provided by a fixture.\n\n    Asserts\n    -------\n    The function raises a ValueError for lists with differing elements\n    and lengths.\n    \"\"\"\n    with pytest.raises(ValueError) as excinfo:\n        validate_sublists(list_with_unequal_elements_and_unbalance)\n    assert \"Sublists do not have the same elements.\" in str(excinfo.value)\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_equal_elements_different_order","title":"<code>list_with_equal_elements_different_order()</code>","text":"<p>Provide a list where sublists have identical elements in different orders.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_equal_elements_different_order--returns","title":"Returns","text":"<p>list     A list of sublists with equal elements in varying order.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>@pytest.fixture()\ndef list_with_equal_elements_different_order():\n    \"\"\"Provide a list where sublists have identical elements in different\n    orders.\n\n    Returns\n    -------\n    list\n        A list of sublists with equal elements in varying order.\n    \"\"\"\n    return [\n        [\"A\", \"B\"],\n        [\"B\", \"A\"],\n    ]\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_equal_elements_equal_order","title":"<code>list_with_equal_elements_equal_order()</code>","text":"<p>Provide a list where sublists have identical elements in the same order.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_equal_elements_equal_order--returns","title":"Returns","text":"<p>list     A list of sublists with equal elements in identical order.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>@pytest.fixture()\ndef list_with_equal_elements_equal_order():\n    \"\"\"Provide a list where sublists have identical elements in the same order.\n\n    Returns\n    -------\n    list\n        A list of sublists with equal elements in identical order.\n    \"\"\"\n    return [\n        [\"A\", \"B\"],\n        [\"A\", \"B\"],\n    ]\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_unequal_elements","title":"<code>list_with_unequal_elements()</code>","text":"<p>Provide a list where sublists have different elements.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_unequal_elements--returns","title":"Returns","text":"<p>list     A list of sublists with unequal elements.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>@pytest.fixture()\ndef list_with_unequal_elements():\n    \"\"\"Provide a list where sublists have different elements.\n\n    Returns\n    -------\n    list\n        A list of sublists with unequal elements.\n    \"\"\"\n    return [\n        [\"A\", \"B\"],\n        [\"C\", \"A\"],\n    ]\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_unequal_elements_and_unbalance","title":"<code>list_with_unequal_elements_and_unbalance()</code>","text":"<p>Provide a list where sublists have different elements and lengths.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.list_with_unequal_elements_and_unbalance--returns","title":"Returns","text":"<p>list     A list of sublists with unequal elements and varying lengths.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>@pytest.fixture()\ndef list_with_unequal_elements_and_unbalance():\n    \"\"\"Provide a list where sublists have different elements and lengths.\n\n    Returns\n    -------\n    list\n        A list of sublists with unequal elements and varying lengths.\n    \"\"\"\n    return [\n        [\"A\", \"B\", \"C\"],\n        [\"C\", \"A\"],\n    ]\n</code></pre>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.test_data_validation","title":"<code>test_data_validation(tmp_path_factory, data_raw, file_name_log, data_schema, data_clean)</code>","text":"<p>Validate the functionality of validate_data_quality.</p> <p>This test iterates through a series of predefined datasets, including both valid and invalid data, to verify the data quality validation process. It checks whether the function correctly processes valid data, identifies invalid data, and logs errors as expected.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.test_data_validation--parameters","title":"Parameters","text":"<p>tmp_path_factory : _pytest.tmpdir.TempPathFactory     A fixture provided by pytest to create temporary directories. data_raw : dict     The raw data dictionary to be validated. Represents a single row of     data intended for processing by the validate_data_quality function. file_name_log : str     The name of the log file used to record validation errors. data_schema : BaseModel     The Pydantic model that the raw data is validated against. data_clean : list     The expected processed data outcome from the validation function,     for comparison with the actual result.</p>"},{"location":"tests/tools/test_validators/#tests.tools.test_validators.test_data_validation--asserts","title":"Asserts","text":"<p>Asserts that the processed data matches the expected data_clean list. Additionally, it checks if the log file's existence aligns with the presence of invalid data, ensuring that logs are created only when there are validation errors.</p> Source code in <code>tests/tools/test_validators.py</code> <pre><code>@pytest.mark.parametrize(\n    \"data_raw, file_name_log, data_schema, data_clean\",\n    data_for_validation_parameters,\n)\ndef test_data_validation(\n    tmp_path_factory,\n    data_raw,\n    file_name_log,\n    data_schema,\n    data_clean,\n):\n    \"\"\"Validate the functionality of validate_data_quality.\n\n    This test iterates through a series of predefined datasets, including\n    both valid and invalid data, to verify the data quality validation\n    process. It checks whether the function correctly processes valid data,\n    identifies invalid data, and logs errors as expected.\n\n    Parameters\n    ----------\n    tmp_path_factory : _pytest.tmpdir.TempPathFactory\n        A fixture provided by pytest to create temporary directories.\n    data_raw : dict\n        The raw data dictionary to be validated. Represents a single row of\n        data intended for processing by the validate_data_quality function.\n    file_name_log : str\n        The name of the log file used to record validation errors.\n    data_schema : BaseModel\n        The Pydantic model that the raw data is validated against.\n    data_clean : list\n        The expected processed data outcome from the validation function,\n        for comparison with the actual result.\n\n    Asserts\n    -------\n    Asserts that the processed data matches the expected data_clean list.\n    Additionally, it checks if the log file's existence aligns with the\n    presence of invalid data, ensuring that logs are created only when there\n    are validation errors.\n    \"\"\"\n    df = pd.DataFrame(\n        data_raw,\n        index=[0],\n        dtype=str,\n    )\n\n    output_dir = tmp_path_factory.mktemp(\"data_validation\")\n\n    data_process = list(\n        validate_data_quality(\n            df,\n            str(output_dir),\n            file_name_log,\n            data_schema,\n        ),\n    )\n\n    if data_process:\n        data_process_value = list(data_process[0].values())\n        is_bad_data = False\n    else:\n        data_process_value = []\n        is_bad_data = True\n\n    log_output = \"test_invalid_records.log\"\n    output_empty_file_path = output_dir / log_output\n\n    assert data_process_value == data_clean\n    assert os.path.exists(output_empty_file_path) == is_bad_data\n</code></pre>"}]}